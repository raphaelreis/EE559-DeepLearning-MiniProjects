{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from torch import nn \n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from utils.new_loader import load,PairSetMNIST,Training_set,Test_set, Training_set_split,Validation_set\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8000, 2, 14, 14])\n",
      "torch.Size([1000, 2, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "# load the dataset as a Dataset object\n",
    "data = PairSetMNIST( rotate=True,translate=False,swap_channel = True)\n",
    "train_data = Training_set(data)\n",
    "test_data = Test_set(data)\n",
    "print(train_data.train_input.shape)\n",
    "print(test_data.test_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module) :\n",
    "    \"\"\"\n",
    "    basic 2d convolution with batch norm\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels,out_channels,kernel_size = 1,stride =1, padding = 0) :\n",
    "        super(conv_block,self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels,out_channels,kernel_size,stride ,padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "    \n",
    "    def forward(self,x) :\n",
    "        x = self.bn(self.conv(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Inception block with four different filters scale\n",
    "    \"\"\"\n",
    "    def __init__(self,in_channels,channels_1x1,channels_3x3,channels_5x5,pool_channels):\n",
    "        super(Inception_block, self).__init__()\n",
    "        # 1x1 convolution\n",
    "        self.conv1x1 = conv_block(in_channels,channels_1x1, kernel_size = 1)\n",
    "        # 3x3 convolution factorized in 1x3 followed by 3x1\n",
    "        self.conv3x3 = nn.Sequential(conv_block(in_channels,channels_3x3, kernel_size = 1),\n",
    "                                     conv_block(channels_3x3, channels_3x3, kernel_size = (1,3), padding = (0,1)),\n",
    "                                     conv_block(channels_3x3, channels_3x3, kernel_size = (3,1), padding = (1,0)))\n",
    "        # 5x5 convolution factorized in two consecutive 3x3 implemented as above\n",
    "        self.conv5x5 = nn.Sequential(conv_block(in_channels,channels_5x5, kernel_size = 1),\n",
    "                                     conv_block(channels_5x5, channels_5x5, kernel_size = (1,3),padding =(0,1)),\n",
    "                                     conv_block(channels_5x5, channels_5x5, kernel_size = (3,1), padding = (1,0)),\n",
    "                                     conv_block(channels_5x5,channels_5x5, kernel_size = (1,3),padding=(0,1)),\n",
    "                                     conv_block(channels_5x5, channels_5x5, kernel_size = (3,1),padding = (1,0)))\n",
    "        # pooling layer \n",
    "        self.pool = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                                  conv_block(in_channels, pool_channels, kernel_size=1))\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # compute the four filter of the inception block :  Nx64x14x14\n",
    "        scale1 = F.relu(self.conv1x1(x))\n",
    "        scale2 = F.relu(self.conv3x3(x))\n",
    "        scale3 = F.relu(self.conv5x5(x))\n",
    "        scale4 = F.relu(self.pool(x))\n",
    "        \n",
    "        # concatenate layer for next result\n",
    "        outputs = [scale1, scale2, scale3, scale4]\n",
    "        # Nx256x14x14\n",
    "        filter_cat = torch.cat(outputs,1)\n",
    "        \n",
    "        return filter_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Auxiliary_loss (nn.Module) :\n",
    "    \"\"\"\n",
    "    auxiliary loss classification of the digit\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,in_channels,drop_prob_aux,nb_classes = 10):\n",
    "        super(Auxiliary_loss, self).__init__()\n",
    "        \n",
    "        self.conv = conv_block(in_channels, 128, kernel_size=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, nb_classes)\n",
    "        self.dropout= nn.Dropout(drop_prob_aux)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # aux: N x 256 x 14 x 14\n",
    "        x = F.adaptive_avg_pool2d(x, (4, 4))\n",
    "        # aux: N x 256 x 4 x 4\n",
    "        x = self.conv(x)\n",
    "        # N x 128 x 4 x 4\n",
    "        x = x.view(-1,2048)\n",
    "        # N x 2048\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # N x 1024\n",
    "        x = self.dropout(x)\n",
    "        # N x 10 (nb_classes)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Google_Net (nn.Module) :\n",
    "    \"\"\"\n",
    "    Google net implementing two inception layer in parralel for each channel\n",
    "    Use auxiliary loss to classify the digit number\n",
    "    Concatenate the number classification feature map and classify the two channel\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,channels_1x1,channels_3x3,channels_5x5,pool_channels,nhidden = 60,\n",
    "                 drop_prob = 0,drop_prob_aux = 0.7,nb_classes = 10):\n",
    "        super(Google_Net, self).__init__()\n",
    "        \n",
    "        # local response norm\n",
    "        #self.conv1 = conv_block(1, 32, kernel_size = 3, padding = (3 - 1)//2)\n",
    "        #inception block\n",
    "        self.inception = Inception_block(1,channels_1x1,channels_3x3,channels_5x5,pool_channels)\n",
    "        #auxiliary\n",
    "        self.auxiliary = Auxiliary_loss(256,drop_prob_aux)\n",
    "        \n",
    "        # weights for binary classification \n",
    "        self.fc1 = nn.Linear(20, nhidden)\n",
    "        self.fc2 = nn.Linear(nhidden, 90)\n",
    "        self.fc3 = nn.Linear(90, 2)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        \n",
    "        # split the 2-channel input into two 1*14*14 images\n",
    "        x = input_[:, 0, :, :].view(-1, 1, 14, 14)\n",
    "        y = input_[:, 1, :, :].view(-1, 1, 14, 14)\n",
    "        \n",
    "        # inception blocks\n",
    "        x = self.inception(x)\n",
    "        y = self.inception(y)\n",
    "        \n",
    "        \n",
    "        # auxiliary loss \n",
    "        x = self.auxiliary(x)\n",
    "        y = self.auxiliary(y)\n",
    "        \n",
    "        # concatenate layers  \n",
    "        z = torch.cat([x, y], 1)\n",
    "        \n",
    "        z = F.relu(self.fc1(z))\n",
    "        z = F.relu(self.fc2(z))\n",
    "        z = self.fc3(z)\n",
    "        \n",
    "        \n",
    "        return x,y,z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### train function ######\n",
    "\n",
    "def train_aux (model, train_data, mini_batch_size=100, optimizer = optim.SGD,\n",
    "                criterion = nn.CrossEntropyLoss(), n_epochs=50, eta=1e-1, lambda_l2 = 0, alpha=0.5, beta=0.5):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Train network with auxiliary loss + weight sharing\n",
    "    \n",
    "    \"\"\"\n",
    "    # create data loader\n",
    "    train_loader = DataLoader(train_data, batch_size=mini_batch_size, shuffle=True)\n",
    "    \n",
    "    model.train()\n",
    "    optimizer = optimizer(model.parameters(), lr = eta)\n",
    "    \n",
    "    for e in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            \n",
    "            input_, target_, classes_ = data\n",
    "            class_1, class_2,out= model(input_)\n",
    "            aux_loss1 = criterion(class_1, classes_[:,0])\n",
    "            aux_loss2 = criterion(class_2, classes_[:,1])\n",
    "            out_loss  = criterion(out, target_)\n",
    "            net_loss = (alpha * (out_loss) + beta * (aux_loss1 + aux_loss2 ))\n",
    "            epoch_loss += net_loss\n",
    "            \n",
    "            if lambda_l2 != 0:\n",
    "                for p in model.parameters():\n",
    "                    epoch_loss += lambda_l2 * p.pow(2).sum() # add an l2 penalty term to the loss \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            net_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print('Train Epoch: {}  | Loss {:.6f}'.format(\n",
    "                e, epoch_loss.item()))\n",
    "        \n",
    "#########################################################################################################################\n",
    "#########################################################################################################################\n",
    "\n",
    "### test function  ###\n",
    "\n",
    "def test_aux(model, test_data, mini_batch_size=100, criterion = nn.CrossEntropyLoss()):\n",
    "    \n",
    "    \"\"\"\n",
    "    Test function to calculate prediction accuracy of a cnn with auxiliary loss\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # create test laoder\n",
    "    test_loader = DataLoader(test_data, batch_size=mini_batch_size, shuffle=True)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    nb_errors=0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            input_, target_, classes_ = data\n",
    "            \n",
    "            _,_ ,output = model(input_) \n",
    "            batch_loss = criterion(output, target_)\n",
    "            test_loss += batch_loss\n",
    "            \n",
    "            _, predicted_classes = output.max(1)\n",
    "            for k in range(mini_batch_size):\n",
    "                if target_[k] != predicted_classes[k]:\n",
    "                    nb_errors = nb_errors + 1\n",
    "                                   \n",
    "             \n",
    "        print('\\nTest set | Loss: {:.4f} | Accuracy: {:.0f}% | # misclassified : {}/{}\\n'.format(\n",
    "        test_loss.item(), 100 * (len(test_data)-nb_errors)/len(test_data), nb_errors, len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0  | Loss 31.377213\n",
      "Train Epoch: 1  | Loss 14.407424\n",
      "Train Epoch: 2  | Loss 8.786600\n",
      "Train Epoch: 3  | Loss 7.054515\n",
      "Train Epoch: 4  | Loss 6.078754\n",
      "Train Epoch: 5  | Loss 4.966668\n",
      "Train Epoch: 6  | Loss 4.498782\n",
      "Train Epoch: 7  | Loss 3.953635\n",
      "Train Epoch: 8  | Loss 3.586557\n",
      "Train Epoch: 9  | Loss 3.155195\n",
      "Train Epoch: 10  | Loss 2.932753\n",
      "Train Epoch: 11  | Loss 2.517632\n",
      "Train Epoch: 12  | Loss 2.422868\n",
      "Train Epoch: 13  | Loss 2.178124\n",
      "Train Epoch: 14  | Loss 1.695058\n",
      "Train Epoch: 15  | Loss 1.768368\n",
      "Train Epoch: 16  | Loss 1.467569\n",
      "Train Epoch: 17  | Loss 1.291992\n",
      "Train Epoch: 18  | Loss 1.166596\n",
      "Train Epoch: 19  | Loss 1.179637\n",
      "Train Epoch: 20  | Loss 1.044233\n",
      "Train Epoch: 21  | Loss 0.977379\n",
      "Train Epoch: 22  | Loss 0.755890\n",
      "Train Epoch: 23  | Loss 0.773148\n",
      "Train Epoch: 24  | Loss 0.718674\n",
      "Train Epoch: 25  | Loss 0.693958\n",
      "Train Epoch: 26  | Loss 0.651033\n",
      "Train Epoch: 27  | Loss 0.645568\n",
      "Train Epoch: 28  | Loss 0.547370\n",
      "Train Epoch: 29  | Loss 0.650572\n",
      "Train Epoch: 30  | Loss 0.468762\n",
      "Train Epoch: 31  | Loss 0.389891\n",
      "Train Epoch: 32  | Loss 0.275721\n",
      "Train Epoch: 33  | Loss 0.329106\n",
      "Train Epoch: 34  | Loss 0.292067\n",
      "Train Epoch: 35  | Loss 0.439112\n",
      "Train Epoch: 36  | Loss 0.365306\n",
      "Train Epoch: 37  | Loss 0.232145\n",
      "Train Epoch: 38  | Loss 0.226339\n",
      "Train Epoch: 39  | Loss 0.318280\n",
      "Train Epoch: 40  | Loss 0.326880\n",
      "Train Epoch: 41  | Loss 0.234762\n",
      "Train Epoch: 42  | Loss 0.237128\n",
      "Train Epoch: 43  | Loss 0.283606\n",
      "Train Epoch: 44  | Loss 0.318300\n",
      "Train Epoch: 45  | Loss 0.206149\n",
      "Train Epoch: 46  | Loss 0.218691\n",
      "Train Epoch: 47  | Loss 0.137301\n",
      "Train Epoch: 48  | Loss 0.183156\n",
      "Train Epoch: 49  | Loss 0.299627\n",
      "\n",
      "Test set | Loss: 1.3979 | Accuracy: 96% | # misclassified : 37/1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Google_Net(64,64,64,64)\n",
    "train_aux(model, train_data)\n",
    "test_aux(model,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0  | Loss 32.618233\n",
      "Train Epoch: 1  | Loss 13.434915\n",
      "Train Epoch: 2  | Loss 8.934690\n",
      "Train Epoch: 3  | Loss 6.789910\n",
      "Train Epoch: 4  | Loss 5.602638\n",
      "Train Epoch: 5  | Loss 4.740703\n",
      "Train Epoch: 6  | Loss 4.155932\n",
      "Train Epoch: 7  | Loss 3.976521\n",
      "Train Epoch: 8  | Loss 3.275844\n",
      "Train Epoch: 9  | Loss 2.913592\n",
      "Train Epoch: 10  | Loss 2.519236\n",
      "Train Epoch: 11  | Loss 2.622946\n",
      "Train Epoch: 12  | Loss 2.283235\n",
      "Train Epoch: 13  | Loss 1.969821\n",
      "Train Epoch: 14  | Loss 1.843643\n",
      "Train Epoch: 15  | Loss 1.550166\n",
      "Train Epoch: 16  | Loss 1.426786\n",
      "Train Epoch: 17  | Loss 1.288421\n",
      "Train Epoch: 18  | Loss 1.332516\n",
      "Train Epoch: 19  | Loss 1.089396\n",
      "Train Epoch: 20  | Loss 0.837815\n",
      "Train Epoch: 21  | Loss 0.964031\n",
      "Train Epoch: 22  | Loss 0.834835\n",
      "Train Epoch: 23  | Loss 0.781474\n",
      "Train Epoch: 24  | Loss 0.603094\n",
      "Train Epoch: 25  | Loss 0.505649\n",
      "Train Epoch: 26  | Loss 0.469993\n",
      "Train Epoch: 27  | Loss 0.526265\n",
      "Train Epoch: 28  | Loss 0.551350\n",
      "Train Epoch: 29  | Loss 0.502799\n",
      "Train Epoch: 30  | Loss 0.461283\n",
      "Train Epoch: 31  | Loss 0.512370\n",
      "Train Epoch: 32  | Loss 0.549370\n",
      "Train Epoch: 33  | Loss 0.464221\n",
      "Train Epoch: 34  | Loss 0.309225\n",
      "Train Epoch: 35  | Loss 0.337185\n",
      "Train Epoch: 36  | Loss 0.283666\n",
      "Train Epoch: 37  | Loss 0.375707\n",
      "Train Epoch: 38  | Loss 0.285451\n",
      "Train Epoch: 39  | Loss 0.313468\n",
      "Train Epoch: 40  | Loss 0.277481\n",
      "Train Epoch: 41  | Loss 0.223018\n",
      "Train Epoch: 42  | Loss 0.285726\n",
      "Train Epoch: 43  | Loss 0.202486\n",
      "Train Epoch: 44  | Loss 0.319305\n",
      "Train Epoch: 45  | Loss 0.223753\n",
      "Train Epoch: 46  | Loss 0.153822\n",
      "Train Epoch: 47  | Loss 0.180249\n",
      "Train Epoch: 48  | Loss 0.323580\n",
      "Train Epoch: 49  | Loss 0.170846\n",
      "\n",
      "Test set | Loss: 0.9704 | Accuracy: 98% | # misclassified : 24/1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2 = Google_Net(64,64,64,64)\n",
    "train_aux(model2, train_data)\n",
    "test_aux(model2,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0  | Loss 30.186747\n",
      "Train Epoch: 1  | Loss 14.004303\n",
      "Train Epoch: 2  | Loss 8.854354\n",
      "Train Epoch: 3  | Loss 6.635819\n",
      "Train Epoch: 4  | Loss 6.167346\n",
      "Train Epoch: 5  | Loss 4.947651\n",
      "Train Epoch: 6  | Loss 4.128981\n",
      "Train Epoch: 7  | Loss 3.859995\n",
      "Train Epoch: 8  | Loss 3.362609\n",
      "Train Epoch: 9  | Loss 2.963300\n",
      "Train Epoch: 10  | Loss 2.969986\n",
      "Train Epoch: 11  | Loss 2.424399\n",
      "Train Epoch: 12  | Loss 2.077563\n",
      "Train Epoch: 13  | Loss 1.753907\n",
      "Train Epoch: 14  | Loss 1.533150\n",
      "Train Epoch: 15  | Loss 1.809114\n",
      "Train Epoch: 16  | Loss 1.479668\n",
      "Train Epoch: 17  | Loss 1.156908\n",
      "Train Epoch: 18  | Loss 1.062603\n",
      "Train Epoch: 19  | Loss 1.073249\n",
      "Train Epoch: 20  | Loss 0.818271\n",
      "Train Epoch: 21  | Loss 0.886532\n",
      "Train Epoch: 22  | Loss 0.813413\n",
      "Train Epoch: 23  | Loss 0.629606\n",
      "Train Epoch: 24  | Loss 0.665872\n",
      "Train Epoch: 25  | Loss 0.675358\n",
      "Train Epoch: 26  | Loss 0.592510\n",
      "Train Epoch: 27  | Loss 0.556999\n",
      "Train Epoch: 28  | Loss 0.626615\n",
      "Train Epoch: 29  | Loss 0.527285\n",
      "Train Epoch: 30  | Loss 0.478520\n",
      "Train Epoch: 31  | Loss 0.497538\n",
      "Train Epoch: 32  | Loss 0.358419\n",
      "Train Epoch: 33  | Loss 0.522586\n",
      "Train Epoch: 34  | Loss 0.509907\n",
      "Train Epoch: 35  | Loss 0.320814\n",
      "Train Epoch: 36  | Loss 0.368942\n",
      "Train Epoch: 37  | Loss 0.339836\n",
      "Train Epoch: 38  | Loss 0.321065\n",
      "Train Epoch: 39  | Loss 0.262444\n",
      "Train Epoch: 40  | Loss 0.300852\n",
      "Train Epoch: 41  | Loss 0.373163\n",
      "Train Epoch: 42  | Loss 0.179234\n",
      "Train Epoch: 43  | Loss 0.194249\n",
      "Train Epoch: 44  | Loss 0.245958\n",
      "Train Epoch: 45  | Loss 0.218180\n",
      "Train Epoch: 46  | Loss 0.149084\n",
      "Train Epoch: 47  | Loss 0.168414\n",
      "Train Epoch: 48  | Loss 0.165086\n",
      "Train Epoch: 49  | Loss 0.172088\n",
      "\n",
      "Test set | Loss: 1.2778 | Accuracy: 96% | # misclassified : 35/1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model3 = Google_Net(64,64,64,64)\n",
    "train_aux(model3, train_data)\n",
    "test_aux(model3,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0  | Loss 104.899452\n",
      "Train Epoch: 1  | Loss 45.052601\n",
      "Train Epoch: 2  | Loss 31.685064\n",
      "Train Epoch: 3  | Loss 25.729883\n",
      "Train Epoch: 4  | Loss 21.673063\n",
      "Train Epoch: 5  | Loss 18.168068\n",
      "Train Epoch: 6  | Loss 15.400216\n",
      "Train Epoch: 7  | Loss 13.026170\n",
      "Train Epoch: 8  | Loss 11.510374\n",
      "Train Epoch: 9  | Loss 10.211856\n",
      "Train Epoch: 10  | Loss 8.992868\n",
      "Train Epoch: 11  | Loss 8.161642\n",
      "Train Epoch: 12  | Loss 7.570150\n",
      "Train Epoch: 13  | Loss 6.382459\n",
      "Train Epoch: 14  | Loss 5.506278\n",
      "Train Epoch: 15  | Loss 5.625072\n",
      "Train Epoch: 16  | Loss 5.564719\n",
      "Train Epoch: 17  | Loss 5.208803\n",
      "Train Epoch: 18  | Loss 4.632640\n",
      "Train Epoch: 19  | Loss 3.732130\n",
      "Train Epoch: 20  | Loss 3.632525\n",
      "Train Epoch: 21  | Loss 3.484497\n",
      "Train Epoch: 22  | Loss 2.767345\n",
      "Train Epoch: 23  | Loss 2.949233\n",
      "Train Epoch: 24  | Loss 2.331871\n",
      "Train Epoch: 25  | Loss 2.440623\n",
      "Train Epoch: 26  | Loss 2.724271\n",
      "Train Epoch: 27  | Loss 2.507766\n",
      "Train Epoch: 28  | Loss 2.288896\n",
      "Train Epoch: 29  | Loss 2.249889\n",
      "Train Epoch: 30  | Loss 2.032427\n",
      "Train Epoch: 31  | Loss 1.976386\n",
      "Train Epoch: 32  | Loss 1.646452\n",
      "Train Epoch: 33  | Loss 1.648767\n",
      "Train Epoch: 34  | Loss 1.404064\n",
      "Train Epoch: 35  | Loss 1.590953\n",
      "Train Epoch: 36  | Loss 1.604782\n",
      "Train Epoch: 37  | Loss 1.482648\n",
      "Train Epoch: 38  | Loss 1.383262\n",
      "Train Epoch: 39  | Loss 1.498524\n",
      "Train Epoch: 40  | Loss 1.289867\n",
      "Train Epoch: 41  | Loss 1.346536\n",
      "Train Epoch: 42  | Loss 1.295486\n",
      "Train Epoch: 43  | Loss 1.294079\n",
      "Train Epoch: 44  | Loss 1.113168\n",
      "Train Epoch: 45  | Loss 0.942775\n",
      "Train Epoch: 46  | Loss 1.179978\n",
      "Train Epoch: 47  | Loss 1.164984\n",
      "Train Epoch: 48  | Loss 0.982220\n",
      "Train Epoch: 49  | Loss 1.131712\n",
      "\n",
      "Test set | Loss: 1145.0746 | Accuracy: 75% | # misclassified : 246/1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model4 = Google_Net(64,64,64,64)\n",
    "train_aux(model4, train_data)\n",
    "test_aux(model4,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
