{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 1\n",
    "import torch \n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from torch import nn \n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from utils.loader import load\n",
    "from utils.loader import PairSetMNIST\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from models.vince_models import Net2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "\n",
    "train_data = PairSetMNIST()\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline models\n",
    "\n",
    "class Net2C(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Network which takes input as a two channel 14*14 image\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nb_hidden):\n",
    "        \n",
    "        super(Net2C, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size = 5) \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 3)\n",
    "        self.fc1 = nn.Linear(256, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 2) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # forward pass\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size = 3, stride = 1))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size = 3, stride = 3))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "    \n",
    "        return x \n",
    "\n",
    "# inspired from LeNet5 but ssingle input image (concatenation of two channels)\n",
    "\n",
    "class Netcat(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Network which processes the input to get a  : 1000 * 1 * 14 * 28 set \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        \n",
    "        super(Netcat, self).__init__()\n",
    "        self.d = dim\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size = 3, stride = 1)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size = 3, stride = 1)\n",
    "        self.fc1 = nn.Linear(480, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)   \n",
    "  \n",
    "        \n",
    "    def forward(self, x):\n",
    "   \n",
    "        x = torch.cat((x[:,0], x[:,1]), dim = 2).unsqueeze(dim = self.d)   # concatenate channels into 1 channel (input : 1000 * 1 * 14 * 28)\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size = 2, stride = 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size = 2, stride = 1))\n",
    "        x = F.relu(self.fc1(x.view(-1, 480)))\n",
    "        x = F.relu(self.fc2(x.view(-1, 120)))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_binary (model, train_loader, optimizer = optim.SGD,\n",
    "                criterion = nn.CrossEntropyLoss(), n_epochs=50, mini_batch_size=100, eta=1e-1, lambda_l2 = 0):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Train network with auxiliary loss + weight sharing\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    optimizer = optimizer(model.parameters(), lr = eta)\n",
    "    \n",
    "    for e in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            \n",
    "            input_, target_ = data\n",
    "\n",
    "            out = model(input_)\n",
    "            out_loss  = criterion(out, target_)\n",
    "            epoch_loss += out_loss\n",
    "            \n",
    "            if lambda_l2 != 0:\n",
    "                for p in model.parameters():\n",
    "                    epoch_loss += lambda_l2 * p.pow(2).sum() # add an l2 penalty term to the loss \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            out_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print('Train Epoch: {}  | Loss {:.6f}'.format(\n",
    "                e, epoch_loss.item()))\n",
    "    \n",
    "# test function #\n",
    "    \n",
    "def test_binary(model, test_input, test_target, mini_batch_size=100, criterion = nn.CrossEntropyLoss()):\n",
    "    \n",
    "    \"\"\"\n",
    "    Test function to calculate prediction accuracy of a cnn with auxiliary loss\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    nb_errors=0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for input_, target_ in zip(test_input.split(mini_batch_size), test_target.split(mini_batch_size)):\n",
    "            \n",
    "            output = model(input_) \n",
    "            batch_loss = criterion(output, target_)\n",
    "            test_loss += batch_loss\n",
    "            \n",
    "            _, predicted_classes = output.max(1)\n",
    "            for k in range(mini_batch_size):\n",
    "                if target_[k] != predicted_classes[k]:\n",
    "                    nb_errors = nb_errors + 1\n",
    "                                   \n",
    "             \n",
    "        print('\\nTest set | Loss: {:.4f} | Accuracy: {:.0f}% | # misclassified : {}/{}\\n'.format(\n",
    "        test_loss.item(), 100 * (len(test_target)-nb_errors)/len(test_target), nb_errors, len(test_target)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0  | Loss 6.799265\n",
      "Train Epoch: 1  | Loss 6.557003\n",
      "Train Epoch: 2  | Loss 6.073422\n",
      "Train Epoch: 3  | Loss 5.996435\n",
      "Train Epoch: 4  | Loss 5.417203\n",
      "Train Epoch: 5  | Loss 5.087328\n",
      "Train Epoch: 6  | Loss 5.180830\n",
      "Train Epoch: 7  | Loss 4.616048\n",
      "Train Epoch: 8  | Loss 4.472587\n",
      "Train Epoch: 9  | Loss 4.350353\n",
      "Train Epoch: 10  | Loss 3.967990\n",
      "Train Epoch: 11  | Loss 4.397269\n",
      "Train Epoch: 12  | Loss 4.072762\n",
      "Train Epoch: 13  | Loss 3.634916\n",
      "Train Epoch: 14  | Loss 4.099465\n",
      "Train Epoch: 15  | Loss 3.031305\n",
      "Train Epoch: 16  | Loss 3.195432\n",
      "Train Epoch: 17  | Loss 3.154566\n",
      "Train Epoch: 18  | Loss 3.577063\n",
      "Train Epoch: 19  | Loss 2.880953\n",
      "Train Epoch: 20  | Loss 1.830984\n",
      "Train Epoch: 21  | Loss 2.838112\n",
      "Train Epoch: 22  | Loss 2.087142\n",
      "Train Epoch: 23  | Loss 1.522783\n",
      "Train Epoch: 24  | Loss 1.850923\n",
      "Train Epoch: 25  | Loss 0.978649\n",
      "Train Epoch: 26  | Loss 2.768861\n",
      "Train Epoch: 27  | Loss 2.337955\n",
      "Train Epoch: 28  | Loss 1.769838\n",
      "Train Epoch: 29  | Loss 0.963205\n",
      "Train Epoch: 30  | Loss 1.103728\n",
      "Train Epoch: 31  | Loss 1.668798\n",
      "Train Epoch: 32  | Loss 0.468763\n",
      "Train Epoch: 33  | Loss 0.250480\n",
      "Train Epoch: 34  | Loss 0.185536\n",
      "Train Epoch: 35  | Loss 0.154536\n",
      "Train Epoch: 36  | Loss 0.115961\n",
      "Train Epoch: 37  | Loss 0.099504\n",
      "Train Epoch: 38  | Loss 0.086294\n",
      "Train Epoch: 39  | Loss 0.074739\n",
      "Train Epoch: 40  | Loss 0.066877\n",
      "Train Epoch: 41  | Loss 0.060191\n",
      "Train Epoch: 42  | Loss 0.054789\n",
      "Train Epoch: 43  | Loss 0.051198\n",
      "Train Epoch: 44  | Loss 0.045471\n",
      "Train Epoch: 45  | Loss 0.042486\n",
      "Train Epoch: 46  | Loss 0.039044\n",
      "Train Epoch: 47  | Loss 0.036376\n",
      "Train Epoch: 48  | Loss 0.034125\n",
      "Train Epoch: 49  | Loss 0.032113\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "###### Binary Classifier ######\n",
    "###############################\n",
    "\n",
    "model_1 = Net2C(200)\n",
    "train_binary(model_1,train_loader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-94419fe6501e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_binary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_input' is not defined"
     ]
    }
   ],
   "source": [
    "test_binary(model_1, test_input, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
