{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 1\n",
    "import torch \n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from torch import nn \n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from utils.loader import load\n",
    "from utils.loader import PairSetMNIST\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset as a Dataset object\n",
    "train_data = PairSetMNIST(train=True)\n",
    "test_data  = PairSetMNIST(test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architectures \n",
    "\n",
    "class LeNet_WS_sequential(nn.Module):\n",
    "    \"\"\"\n",
    "    Weight sharing \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(LeNet_WS_sequential, self).__init__()\n",
    "        \n",
    "        # convolutional weights for digit reocgnition shared for each image\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(256, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 10)\n",
    "        # fully connected layers \n",
    "        self.fc3 = nn.Linear(20, 60)\n",
    "        self.fc4 = nn.Linear(60, 90)\n",
    "        self.fc5 = nn.Linear(90, 2)\n",
    "        \n",
    "    def forward(self, input_):        \n",
    "        \n",
    "        # split the 2-channel input into two 14*14 images\n",
    "        x = input_[:, 0, :, :].view(-1, 1, 14, 14)\n",
    "        y = input_[:, 1, :, :].view(-1, 1, 14, 14)\n",
    "        \n",
    "        # forward pass for the first image \n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        # forward pass for the second image\n",
    "        y = F.relu(F.max_pool2d(self.conv1(y), kernel_size=2, stride=2))\n",
    "        y = F.relu(F.max_pool2d(self.conv2(y), kernel_size=2, stride=2))\n",
    "        y = F.relu(self.fc1(y.view(-1, 256)))\n",
    "        y = self.fc2(y)\n",
    "        \n",
    "        # concatenate layers \n",
    "        z = torch.cat([x, y], 1)\n",
    "        \n",
    "        z = F.relu(self.fc3(z))\n",
    "        z = F.relu(self.fc4(z))\n",
    "        z = self.fc5(z)\n",
    "        \n",
    "        return  z\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### train function ######\n",
    "\n",
    "def train_WS (model, train_data, mini_batch_size=100, optimizer = optim.SGD,\n",
    "                criterion = nn.CrossEntropyLoss(), n_epochs=50, eta=1e-1, lambda_l2 = 0):\n",
    "    \n",
    "    \"\"\"\n",
    "    Train network with simple weight sharing\n",
    "    \n",
    "    \"\"\"\n",
    "    # create data loader\n",
    "    train_loader = DataLoader(train_data, batch_size=mini_batch_size, shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    optimizer = optimizer(model.parameters(), lr = eta)\n",
    "    \n",
    "    for e in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            \n",
    "            input_, target_, classes_ = data\n",
    "            \n",
    "            out_loss  = criterion(model(input_), target_)\n",
    "            epoch_loss += out_loss\n",
    "            \n",
    "            if lambda_l2 != 0:\n",
    "                for p in model.parameters():\n",
    "                    epoch_loss += lambda_l2 * p.pow(2).sum() # add an l2 penalty term to the loss \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            out_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print('Train Epoch: {}  | Loss {:.6f}'.format(\n",
    "                e, epoch_loss.item()))\n",
    "        \n",
    "#########################################################################################################################\n",
    "#########################################################################################################################\n",
    "\n",
    "### test function  ###\n",
    "\n",
    "def test_WS (model, test_data, mini_batch_size=100, criterion = nn.CrossEntropyLoss()):\n",
    "    \n",
    "    \"\"\"\n",
    "    Test function to calculate prediction accuracy of a binary cnn classifier\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # create test laoder\n",
    "    test_loader = DataLoader(test_data, batch_size=mini_batch_size, shuffle=True)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    nb_errors=0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            input_, target_, classes_ = data\n",
    "            output = model(input_) \n",
    "            batch_loss = criterion(output, target_)\n",
    "            test_loss += batch_loss\n",
    "            \n",
    "            _, predicted_classes = output.max(1)\n",
    "            for k in range(mini_batch_size):\n",
    "                if target_[k] != predicted_classes[k]:\n",
    "                    nb_errors = nb_errors + 1\n",
    "                                   \n",
    "             \n",
    "        print('\\nTest set | Loss: {:.4f} | Accuracy: {:.0f}% | # misclassified : {}/{}\\n'.format(\n",
    "        test_loss.item(), 100 * (len(test_data)-nb_errors)/len(test_data), nb_errors, len(test_data)))\n",
    "        \n",
    "        print(nb_errors)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0  | Loss 6.895623\n",
      "Train Epoch: 1  | Loss 6.879736\n",
      "Train Epoch: 2  | Loss 6.874956\n",
      "Train Epoch: 3  | Loss 6.865646\n",
      "Train Epoch: 4  | Loss 6.862055\n",
      "Train Epoch: 5  | Loss 6.848652\n",
      "Train Epoch: 6  | Loss 6.828834\n",
      "Train Epoch: 7  | Loss 6.803328\n",
      "Train Epoch: 8  | Loss 6.760359\n",
      "Train Epoch: 9  | Loss 6.679596\n",
      "Train Epoch: 10  | Loss 6.508476\n",
      "Train Epoch: 11  | Loss 6.164664\n",
      "Train Epoch: 12  | Loss 5.486394\n",
      "Train Epoch: 13  | Loss 4.597522\n",
      "Train Epoch: 14  | Loss 4.025372\n",
      "Train Epoch: 15  | Loss 3.539963\n",
      "Train Epoch: 16  | Loss 3.185930\n",
      "Train Epoch: 17  | Loss 2.914316\n",
      "Train Epoch: 18  | Loss 3.096124\n",
      "Train Epoch: 19  | Loss 2.440158\n",
      "Train Epoch: 20  | Loss 2.000538\n",
      "Train Epoch: 21  | Loss 1.894639\n",
      "Train Epoch: 22  | Loss 1.806071\n",
      "Train Epoch: 23  | Loss 1.604826\n",
      "Train Epoch: 24  | Loss 1.502590\n",
      "Train Epoch: 25  | Loss 1.463696\n",
      "Train Epoch: 26  | Loss 1.429723\n",
      "Train Epoch: 27  | Loss 0.802154\n",
      "Train Epoch: 28  | Loss 0.759072\n",
      "Train Epoch: 29  | Loss 0.370543\n",
      "Train Epoch: 30  | Loss 0.194471\n",
      "Train Epoch: 31  | Loss 0.164100\n",
      "Train Epoch: 32  | Loss 0.173856\n",
      "Train Epoch: 33  | Loss 0.214049\n",
      "Train Epoch: 34  | Loss 0.058961\n",
      "Train Epoch: 35  | Loss 0.031420\n",
      "Train Epoch: 36  | Loss 0.023895\n",
      "Train Epoch: 37  | Loss 0.019793\n",
      "Train Epoch: 38  | Loss 0.016861\n",
      "Train Epoch: 39  | Loss 0.014871\n",
      "Train Epoch: 40  | Loss 0.013308\n",
      "Train Epoch: 41  | Loss 0.011983\n",
      "Train Epoch: 42  | Loss 0.010845\n",
      "Train Epoch: 43  | Loss 0.009858\n",
      "Train Epoch: 44  | Loss 0.009188\n",
      "Train Epoch: 45  | Loss 0.008488\n",
      "Train Epoch: 46  | Loss 0.007882\n",
      "Train Epoch: 47  | Loss 0.007354\n",
      "Train Epoch: 48  | Loss 0.006905\n",
      "Train Epoch: 49  | Loss 0.006495\n",
      "\n",
      "Test set | Loss: 7.0962 | Accuracy: 85% | # misclassified : 151/1000\n",
      "\n",
      "151\n"
     ]
    }
   ],
   "source": [
    "model = LeNet_WS_sequential(500)\n",
    "train_WS(model, train_data)\n",
    "test_WS(model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
