{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 1\n",
    "import torch \n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from torch import nn \n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from utils.loader import load\n",
    "from utils.loader import PairSetMNIST\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.metrics import accuracy, compute_nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet_aux_sequential(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Weight sharing + Auxiliary loss\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(LeNet_aux_sequential, self).__init__()\n",
    "        # convolutional weights for digit reocgnition shared for each image\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "        \n",
    "        # weights for binary classification \n",
    "        self.fc3 = nn.Linear(20, 60)\n",
    "        self.fc4 = nn.Linear(60, 90)\n",
    "        self.fc5 = nn.Linear(90, 2)\n",
    "        \n",
    "    def forward(self, input_):    \n",
    "        \n",
    "        # split the 2-channel input into two 14*14 images\n",
    "        x = input_[:, 0, :, :].view(-1, 1, 14, 14)\n",
    "        y = input_[:, 1, :, :].view(-1, 1, 14, 14)\n",
    "        \n",
    "        # forward pass for the first image \n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # forward pass for the second image \n",
    "        y = F.relu(F.max_pool2d(self.conv1(y), kernel_size=2, stride=2))\n",
    "        y = F.relu(F.max_pool2d(self.conv2(y), kernel_size=2, stride=2))\n",
    "        y = F.relu(self.fc1(y.view(-1, 256)))\n",
    "        y = self.fc2(y)\n",
    "        \n",
    "        # concatenate layers  \n",
    "        z = torch.cat([x, y], 1)\n",
    "        \n",
    "        z = F.relu(self.fc3(z))\n",
    "        z = F.relu(self.fc4(z))\n",
    "        z = self.fc5(z)\n",
    "        \n",
    "        return x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train for one epoch\n",
    "def train_aux (model, train_data, mini_batch_size=100, optimizer = optim.SGD,\n",
    "                criterion = nn.CrossEntropyLoss(), n_epochs=50, eta=1e-1, lambda_l2 = 0, alpha=0.5, beta=0.5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Train network with auxiliary loss + weight sharing\n",
    "    \n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer = optimizer(model.parameters(), lr = eta)\n",
    "    \n",
    "    for e in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        # create data loader\n",
    "        train_loader = DataLoader(train_data, batch_size=mini_batch_size, shuffle=True)\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            \n",
    "            input_, target_, classes_ = data\n",
    "            class_1, class_2, out = model(input_)\n",
    "            aux_loss1 = criterion(class_1, classes_[:,0])\n",
    "            aux_loss2 = criterion(class_2, classes_[:,1])\n",
    "            out_loss  = criterion(out, target_)\n",
    "            net_loss = (alpha * (out_loss) + beta * (aux_loss1 + aux_loss2) )\n",
    "            epoch_loss += net_loss\n",
    "            \n",
    "            if lambda_l2 != 0:\n",
    "                for p in model.parameters():\n",
    "                    epoch_loss += lambda_l2 * p.pow(2).sum() # add an l2 penalty term to the loss \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            net_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print('Train Epoch: {}  | Loss {:.6f}'.format(\n",
    "                e, epoch_loss.item()))\n",
    "        \n",
    "# test on a given test dataset\n",
    "def test_aux(model, test_data, mini_batch_size=100, criterion = nn.CrossEntropyLoss()):\n",
    "    \n",
    "    \"\"\"\n",
    "    Test function to calculate prediction accuracy of a cnn with auxiliary loss\n",
    "    \n",
    "    \"\"\"\n",
    "    test_loader = DataLoader(test_data, batch_size=mini_batch_size, shuffle=True)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    nb_errors = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            input_, target_, classes_ = data\n",
    "            _, _, output = model(input_) \n",
    "            batch_loss = criterion(output, target_)\n",
    "            test_loss += batch_loss     \n",
    "            nb_errors += compute_nb_errors(output, target_)\n",
    "            \n",
    "        acc = 100*(nb_errors/len(test_loader))\n",
    "      \n",
    "                    \n",
    "        print('\\nSet | Loss: {:.4f} | Accuracy: {:.2f}%\\n'.format(test_loss.item(), acc))\n",
    "        \n",
    "        return test_loss, acc\n",
    "\n",
    "# run a model for 10 rounds of train/test and compute results\n",
    "\n",
    "def test_trial(n_trial, n_epochs_):\n",
    "    \n",
    "    train_data = PairSetMNIST(train=True)\n",
    "    test_data  = PairSetMNIST(test=True)\n",
    "\n",
    "    accuracies_test = torch.empty(n_trial, dtype = float)\n",
    "    accuracies_train = torch.empty(n_trial, dtype = float)\n",
    "    losses_train = torch.empty(n_trial, dtype = float)\n",
    "    losses_test = torch.empty(n_trial, dtype = float)\n",
    "\n",
    "    for n in range(n_trial):\n",
    "\n",
    "        model = LeNet_aux_sequential()\n",
    "        model.train(True)\n",
    "        train_aux (model, train_data, n_epochs=n_epochs_ )\n",
    "        test_loss, acc_test = test_aux(model, test_data)\n",
    "        train_loss, acc_train = test_aux(model, train_data)\n",
    "\n",
    "        accuracies_test[n] = acc_test\n",
    "        accuracies_train[n] = acc_train\n",
    "        losses_test[n] = test_loss\n",
    "        losses_train[n] = train_loss\n",
    "        \n",
    "        \n",
    "    return accuracies_train, losses_train, accuracies_test, losses_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0  | Loss 25.795433\n",
      "Train Epoch: 1  | Loss 23.362532\n",
      "Train Epoch: 2  | Loss 19.193148\n",
      "\n",
      "Set | Loss: 6.9607 | Accuracy: 4730.00%\n",
      "\n",
      "\n",
      "Set | Loss: 6.8350 | Accuracy: 4490.00%\n",
      "\n",
      "Train Epoch: 0  | Loss 26.018250\n",
      "Train Epoch: 1  | Loss 24.082714\n",
      "Train Epoch: 2  | Loss 19.878902\n",
      "\n",
      "Set | Loss: 6.9190 | Accuracy: 4760.00%\n",
      "\n",
      "\n",
      "Set | Loss: 6.8686 | Accuracy: 4490.00%\n",
      "\n",
      "Train Epoch: 0  | Loss 25.994595\n",
      "Train Epoch: 1  | Loss 24.088398\n",
      "Train Epoch: 2  | Loss 21.275850\n",
      "\n",
      "Set | Loss: 6.8696 | Accuracy: 4750.00%\n",
      "\n",
      "\n",
      "Set | Loss: 6.8181 | Accuracy: 4490.00%\n",
      "\n",
      "Train Epoch: 0  | Loss 25.696764\n",
      "Train Epoch: 1  | Loss 23.007717\n",
      "Train Epoch: 2  | Loss 19.699543\n",
      "\n",
      "Set | Loss: 6.7992 | Accuracy: 4630.00%\n",
      "\n",
      "\n",
      "Set | Loss: 6.7565 | Accuracy: 4400.00%\n",
      "\n",
      "Train Epoch: 0  | Loss 26.229111\n",
      "Train Epoch: 1  | Loss 25.138542\n",
      "Train Epoch: 2  | Loss 22.811392\n",
      "\n",
      "Set | Loss: 6.9277 | Accuracy: 4760.00%\n",
      "\n",
      "\n",
      "Set | Loss: 6.8706 | Accuracy: 4490.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "######## Auxiliary Loss #######\n",
    "###############################\n",
    "\n",
    "#model = LeNet_aux_sequential()\n",
    "#train_aux(model, train_data)\n",
    "#test_loss, acc = test_aux(model,test_data)\n",
    "accuracies_train, losses_train, accuracies_test, losses_test = test_trial(n_trial=5, n_epochs_=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-7607fdf67071>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPairSetMNIST\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtest_loader\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "test_data = PairSetMNIST(test=True)\n",
    "test_loader = DataLoader(test_data, batch_size=100, shuffle=True)\n",
    "test_loader[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([400., 400., 400., 400., 400.], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(accuracies_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3, 4, 5 ,7, 8, 8])\n",
    "y = torch.tensor([3, 5, 3, 8, 4, 7, 8, 5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3750)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x == y).sum().float()/ ( x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
