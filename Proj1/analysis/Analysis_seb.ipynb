{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 1\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "from utils.loader import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "train_input,train_target, train_classes, test_input, test_target, test_classes = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2, 14, 14])\n",
      "torch.Size([1000])\n",
      "torch.Size([1000, 2])\n",
      "torch.Size([1000, 2, 14, 14])\n",
      "torch.Size([1000])\n",
      "torch.Size([1000, 2])\n"
     ]
    }
   ],
   "source": [
    "print(train_input.shape)\n",
    "print(train_target.shape)\n",
    "print(train_classes.shape)\n",
    "print(test_input.shape)\n",
    "print(test_target.shape)\n",
    "print(test_classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot_labels(target):\n",
    "    tmp = torch.empty(target.shape[0],2)\n",
    "    tmp[:,0] = target\n",
    "    tmp[:,1] = 1 - target\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_one_hot = convert_to_one_hot_labels(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the dataset\n",
    "mean, std = train_input.mean(), train_input.std()\n",
    "\n",
    "train_input = train_input.sub_(mean).div_(std)\n",
    "test_input = test_input.sub_(mean).div_(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size = 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 2)\n",
    "        self.fc1 = nn.Linear(64, 200)\n",
    "        self.fc2 = nn.Linear(200, 50)\n",
    "        self.fc3 = nn.Linear(50, 2)\n",
    "         \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size = 3))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size = 2))\n",
    "        x = F.relu(self.fc1(x.view(-1,64)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_input,train_target, criterion = nn.MSELoss(),\n",
    "          optimizer =optim.SGD,batch_size=100,eta = 0.1, nb_epochs = 100) :\n",
    "    \n",
    "    # Define the loss and optimizing algorithm\n",
    "    optimizer = optimizer(model.parameters(), lr = eta)\n",
    "    # Train the model\n",
    "    for e in range(nb_epochs):\n",
    "        running_loss = 0.0\n",
    "        for input_, target in (zip(train_input.split(batch_size), train_target.split(batch_size))):\n",
    "            output = model(input_)\n",
    "            loss = criterion(output, target)\n",
    "            running_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('Epoch: {}   Loss {:.6f}'.format(e,running_loss ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0   Loss 2.759783\n",
      "Epoch: 1   Loss 2.474296\n",
      "Epoch: 2   Loss 2.451608\n",
      "Epoch: 3   Loss 2.424509\n",
      "Epoch: 4   Loss 2.389571\n",
      "Epoch: 5   Loss 2.346220\n",
      "Epoch: 6   Loss 2.298005\n",
      "Epoch: 7   Loss 2.300551\n",
      "Epoch: 8   Loss 2.317957\n",
      "Epoch: 9   Loss 2.269802\n",
      "Epoch: 10   Loss 2.239126\n",
      "Epoch: 11   Loss 2.204640\n",
      "Epoch: 12   Loss 2.170974\n",
      "Epoch: 13   Loss 2.138062\n",
      "Epoch: 14   Loss 2.111151\n",
      "Epoch: 15   Loss 2.083922\n",
      "Epoch: 16   Loss 2.041439\n",
      "Epoch: 17   Loss 2.039976\n",
      "Epoch: 18   Loss 1.964479\n",
      "Epoch: 19   Loss 1.858805\n",
      "Epoch: 20   Loss 2.009126\n",
      "Epoch: 21   Loss 1.999865\n",
      "Epoch: 22   Loss 1.826746\n",
      "Epoch: 23   Loss 1.746110\n",
      "Epoch: 24   Loss 1.983508\n",
      "Epoch: 25   Loss 1.881182\n",
      "Epoch: 26   Loss 1.751429\n",
      "Epoch: 27   Loss 1.659356\n",
      "Epoch: 28   Loss 1.797182\n",
      "Epoch: 29   Loss 1.871837\n",
      "Epoch: 30   Loss 1.742756\n",
      "Epoch: 31   Loss 1.345182\n",
      "Epoch: 32   Loss 1.930056\n",
      "Epoch: 33   Loss 1.702348\n",
      "Epoch: 34   Loss 1.256894\n",
      "Epoch: 35   Loss 1.859226\n",
      "Epoch: 36   Loss 1.650365\n",
      "Epoch: 37   Loss 1.263670\n",
      "Epoch: 38   Loss 1.509590\n",
      "Epoch: 39   Loss 1.571188\n",
      "Epoch: 40   Loss 1.558620\n",
      "Epoch: 41   Loss 1.561091\n",
      "Epoch: 42   Loss 1.540109\n",
      "Epoch: 43   Loss 1.399998\n",
      "Epoch: 44   Loss 1.436056\n",
      "Epoch: 45   Loss 1.398298\n",
      "Epoch: 46   Loss 1.355347\n",
      "Epoch: 47   Loss 1.397835\n",
      "Epoch: 48   Loss 1.356950\n",
      "Epoch: 49   Loss 1.310171\n",
      "Epoch: 50   Loss 1.329960\n",
      "Epoch: 51   Loss 1.317051\n",
      "Epoch: 52   Loss 1.233469\n",
      "Epoch: 53   Loss 1.251330\n",
      "Epoch: 54   Loss 1.293149\n",
      "Epoch: 55   Loss 1.261208\n",
      "Epoch: 56   Loss 1.252226\n",
      "Epoch: 57   Loss 1.273415\n",
      "Epoch: 58   Loss 1.255754\n",
      "Epoch: 59   Loss 1.189055\n",
      "Epoch: 60   Loss 1.150136\n",
      "Epoch: 61   Loss 1.115701\n",
      "Epoch: 62   Loss 1.174235\n",
      "Epoch: 63   Loss 1.171308\n",
      "Epoch: 64   Loss 1.141888\n",
      "Epoch: 65   Loss 1.116559\n",
      "Epoch: 66   Loss 1.111067\n",
      "Epoch: 67   Loss 1.059184\n",
      "Epoch: 68   Loss 1.098768\n",
      "Epoch: 69   Loss 0.985539\n",
      "Epoch: 70   Loss 0.916286\n",
      "Epoch: 71   Loss 0.943870\n",
      "Epoch: 72   Loss 1.188475\n",
      "Epoch: 73   Loss 1.083383\n",
      "Epoch: 74   Loss 1.105008\n",
      "Epoch: 75   Loss 1.072011\n",
      "Epoch: 76   Loss 0.945908\n",
      "Epoch: 77   Loss 0.705030\n",
      "Epoch: 78   Loss 1.098656\n",
      "Epoch: 79   Loss 1.033904\n",
      "Epoch: 80   Loss 0.995139\n",
      "Epoch: 81   Loss 0.939719\n",
      "Epoch: 82   Loss 0.862604\n",
      "Epoch: 83   Loss 0.949720\n",
      "Epoch: 84   Loss 0.846984\n",
      "Epoch: 85   Loss 0.928376\n",
      "Epoch: 86   Loss 0.955197\n",
      "Epoch: 87   Loss 0.906716\n",
      "Epoch: 88   Loss 0.907012\n",
      "Epoch: 89   Loss 0.909103\n",
      "Epoch: 90   Loss 0.743295\n",
      "Epoch: 91   Loss 0.930061\n",
      "Epoch: 92   Loss 0.661110\n",
      "Epoch: 93   Loss 0.901100\n",
      "Epoch: 94   Loss 0.599103\n",
      "Epoch: 95   Loss 0.830443\n",
      "Epoch: 96   Loss 0.654294\n",
      "Epoch: 97   Loss 0.891011\n",
      "Epoch: 98   Loss 0.568685\n",
      "Epoch: 99   Loss 0.802662\n"
     ]
    }
   ],
   "source": [
    "model = LeNet()\n",
    "train(model,train_input,train_target_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_MSE(model, test_input, test_target ) :\n",
    "    # compute the output of the train model with the test set\n",
    "    test_output = model(test_input)\n",
    "    nb_test_errors = 0\n",
    "    pred = test_output.max(1)[1]# output of the NN is 10 value for each class take the max and the [] stand for the indice of the class\n",
    "    for n in range(test_input.size(0)) :\n",
    "        idx = pred[n]\n",
    "        if test_target[n, idx] < 0.5 : nb_test_errors = nb_test_errors + 1 # if the the predicted value is correct then in the target it is 0.9 o/w 0.0\n",
    "    print('test error Net {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),nb_test_errors, test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_CrossEntropy2(model, data_input, data_target) :\n",
    "    nb_errors,mini_batch_size = 0, 100\n",
    "\n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        output = model(data_input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = output.max(1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if data_target[b + k] != predicted_classes[k]:\n",
    "                nb_errors = nb_errors + 1\n",
    "\n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_CrossEntropy(model, test_input, test_target ) :\n",
    "    # compute the output of the train model with the test set\n",
    "    test_output = model(test_input)\n",
    "    nb_test_errors = 0\n",
    "    pred = test_output.max(1)[1]# output of the NN is 10 value for each class take the max and the [] stand for the indice of the class\n",
    "    for n in range(test_input.size(0)) :\n",
    "        idx = pred[n]\n",
    "        if idx!= test_target[n]: nb_test_errors = nb_test_errors + 1 # if the the predicted value is correct then in the target it is 0.9 o/w 0.0\n",
    "    print('test error Net {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),nb_test_errors, test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net 21.30% 213/1000\n"
     ]
    }
   ],
   "source": [
    "test_target_one_hot = convert_to_one_hot_labels(test_target)\n",
    "nb_test_errors = compute_nb_errors_MSE(model, test_input, test_target_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0   Loss 6.902992\n",
      "Train Epoch: 1   Loss 6.832642\n",
      "Train Epoch: 2   Loss 6.777911\n",
      "Train Epoch: 3   Loss 6.702267\n",
      "Train Epoch: 4   Loss 6.596996\n",
      "Train Epoch: 5   Loss 6.459392\n",
      "Train Epoch: 6   Loss 6.359343\n",
      "Train Epoch: 7   Loss 6.444841\n",
      "Train Epoch: 8   Loss 6.293008\n",
      "Train Epoch: 9   Loss 6.215218\n",
      "Train Epoch: 10   Loss 6.156907\n",
      "Train Epoch: 11   Loss 6.052761\n",
      "Train Epoch: 12   Loss 5.872671\n",
      "Train Epoch: 13   Loss 5.658725\n",
      "Train Epoch: 14   Loss 5.822778\n",
      "Train Epoch: 15   Loss 5.772002\n",
      "Train Epoch: 16   Loss 5.618726\n",
      "Train Epoch: 17   Loss 5.609315\n",
      "Train Epoch: 18   Loss 5.508253\n",
      "Train Epoch: 19   Loss 5.463863\n",
      "Train Epoch: 20   Loss 5.386256\n",
      "Train Epoch: 21   Loss 5.106036\n",
      "Train Epoch: 22   Loss 5.180798\n",
      "Train Epoch: 23   Loss 5.241637\n",
      "Train Epoch: 24   Loss 4.254372\n",
      "Train Epoch: 25   Loss 5.406770\n",
      "Train Epoch: 26   Loss 4.993511\n",
      "Train Epoch: 27   Loss 4.923427\n",
      "Train Epoch: 28   Loss 4.825232\n",
      "Train Epoch: 29   Loss 4.111238\n",
      "Train Epoch: 30   Loss 4.789965\n",
      "Train Epoch: 31   Loss 4.579930\n",
      "Train Epoch: 32   Loss 4.475161\n",
      "Train Epoch: 33   Loss 4.413943\n",
      "Train Epoch: 34   Loss 4.362524\n",
      "Train Epoch: 35   Loss 4.017731\n",
      "Train Epoch: 36   Loss 4.993093\n",
      "Train Epoch: 37   Loss 3.921789\n",
      "Train Epoch: 38   Loss 4.652153\n",
      "Train Epoch: 39   Loss 4.225290\n",
      "Train Epoch: 40   Loss 4.047393\n",
      "Train Epoch: 41   Loss 3.823215\n",
      "Train Epoch: 42   Loss 3.574578\n",
      "Train Epoch: 43   Loss 3.882585\n",
      "Train Epoch: 44   Loss 4.047489\n",
      "Train Epoch: 45   Loss 3.728981\n",
      "Train Epoch: 46   Loss 3.758396\n",
      "Train Epoch: 47   Loss 3.687219\n",
      "Train Epoch: 48   Loss 3.497155\n",
      "Train Epoch: 49   Loss 3.017876\n",
      "Train Epoch: 50   Loss 3.455353\n",
      "Train Epoch: 51   Loss 4.781612\n",
      "Train Epoch: 52   Loss 2.656409\n",
      "Train Epoch: 53   Loss 3.786141\n",
      "Train Epoch: 54   Loss 2.849741\n",
      "Train Epoch: 55   Loss 3.210804\n",
      "Train Epoch: 56   Loss 3.577817\n",
      "Train Epoch: 57   Loss 2.826040\n",
      "Train Epoch: 58   Loss 3.641293\n",
      "Train Epoch: 59   Loss 2.640202\n",
      "Train Epoch: 60   Loss 2.782400\n",
      "Train Epoch: 61   Loss 2.834955\n",
      "Train Epoch: 62   Loss 2.404831\n",
      "Train Epoch: 63   Loss 3.991247\n",
      "Train Epoch: 64   Loss 2.452550\n",
      "Train Epoch: 65   Loss 2.869450\n",
      "Train Epoch: 66   Loss 3.434594\n",
      "Train Epoch: 67   Loss 1.793911\n",
      "Train Epoch: 68   Loss 3.094221\n",
      "Train Epoch: 69   Loss 1.977537\n",
      "Train Epoch: 70   Loss 3.286193\n",
      "Train Epoch: 71   Loss 1.504653\n",
      "Train Epoch: 72   Loss 2.574206\n",
      "Train Epoch: 73   Loss 1.906133\n",
      "Train Epoch: 74   Loss 3.473611\n",
      "Train Epoch: 75   Loss 1.350951\n",
      "Train Epoch: 76   Loss 1.165859\n",
      "Train Epoch: 77   Loss 4.636478\n",
      "Train Epoch: 78   Loss 1.591363\n",
      "Train Epoch: 79   Loss 1.571486\n",
      "Train Epoch: 80   Loss 3.151257\n",
      "Train Epoch: 81   Loss 1.077721\n",
      "Train Epoch: 82   Loss 0.791125\n",
      "Train Epoch: 83   Loss 3.690435\n",
      "Train Epoch: 84   Loss 1.620788\n",
      "Train Epoch: 85   Loss 0.918768\n",
      "Train Epoch: 86   Loss 0.677667\n",
      "Train Epoch: 87   Loss 2.089171\n",
      "Train Epoch: 88   Loss 2.842741\n",
      "Train Epoch: 89   Loss 2.552394\n",
      "Train Epoch: 90   Loss 0.828332\n",
      "Train Epoch: 91   Loss 0.556610\n",
      "Train Epoch: 92   Loss 0.434488\n",
      "Train Epoch: 93   Loss 0.347115\n",
      "Train Epoch: 94   Loss 0.269760\n",
      "Train Epoch: 95   Loss 0.221725\n",
      "Train Epoch: 96   Loss 0.182033\n",
      "Train Epoch: 97   Loss 0.148454\n",
      "Train Epoch: 98   Loss 0.127647\n",
      "Train Epoch: 99   Loss 0.107584\n"
     ]
    }
   ],
   "source": [
    "model1 = LeNet()\n",
    "train (model1,train_input,train_target,nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net 17.30% 173/1000\n"
     ]
    }
   ],
   "source": [
    "nb_test_errors = compute_nb_errors_CrossEntropy(model1, test_input, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, nb_channels, kernel_size,skip_connections = True, batch_normalization = True):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(nb_channels, nb_channels,\n",
    "                               kernel_size = kernel_size,\n",
    "                               padding = (kernel_size - 1) // 2)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(nb_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(nb_channels, nb_channels,\n",
    "                               kernel_size = kernel_size,\n",
    "                               padding = (kernel_size - 1) // 2)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(nb_channels)\n",
    "        \n",
    "        self.skip_connections = skip_connections\n",
    "        self.batch_normalization = batch_normalization\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        if self.batch_normalization: y = self.bn1(y)\n",
    "        y = F.relu(y)\n",
    "        y = self.conv2(y)\n",
    "        if self.batch_normalization: y = self.bn2(y)\n",
    "        if self.skip_connections: y = y + x\n",
    "        y = F.relu(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, nb_residual_blocks, nb_channels,kernel_size = 3, nb_classes = 2,skip_connections = True, batch_normalization = True):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(2, nb_channels,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = (kernel_size - 1) // 2)\n",
    "        self.bn = nn.BatchNorm2d(nb_channels)\n",
    "\n",
    "        self.resnet_blocks = nn.Sequential(\n",
    "            *(ResNetBlock(nb_channels, kernel_size,batch_normalization,skip_connections)\n",
    "              for _ in range(nb_residual_blocks))\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(nb_channels, nb_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn(self.conv(x)))\n",
    "        x = self.resnet_blocks(x)\n",
    "        x = F.avg_pool2d(x, 14).view(-1,32)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0   Loss 46.887879\n",
      "Epoch: 1   Loss 7.820994\n",
      "Epoch: 2   Loss 7.152234\n",
      "Epoch: 3   Loss 6.970767\n",
      "Epoch: 4   Loss 6.840898\n",
      "Epoch: 5   Loss 6.739643\n",
      "Epoch: 6   Loss 7.027691\n",
      "Epoch: 7   Loss 6.992197\n",
      "Epoch: 8   Loss 6.541876\n",
      "Epoch: 9   Loss 6.390764\n",
      "Epoch: 10   Loss 5.930923\n",
      "Epoch: 11   Loss 5.411363\n",
      "Epoch: 12   Loss 5.280518\n",
      "Epoch: 13   Loss 4.926822\n",
      "Epoch: 14   Loss 4.506432\n",
      "Epoch: 15   Loss 4.174598\n",
      "Epoch: 16   Loss 4.007925\n",
      "Epoch: 17   Loss 4.132689\n",
      "Epoch: 18   Loss 3.198866\n",
      "Epoch: 19   Loss 3.770926\n",
      "Epoch: 20   Loss 2.889369\n",
      "Epoch: 21   Loss 2.509427\n",
      "Epoch: 22   Loss 2.483577\n",
      "Epoch: 23   Loss 2.581824\n",
      "Epoch: 24   Loss 2.554514\n",
      "Epoch: 25   Loss 2.371911\n",
      "Epoch: 26   Loss 1.668057\n",
      "Epoch: 27   Loss 1.916489\n",
      "Epoch: 28   Loss 1.030010\n",
      "Epoch: 29   Loss 0.541657\n",
      "Epoch: 30   Loss 1.258018\n",
      "Epoch: 31   Loss 2.342561\n",
      "Epoch: 32   Loss 0.388738\n",
      "Epoch: 33   Loss 0.182891\n",
      "Epoch: 34   Loss 0.101074\n",
      "Epoch: 35   Loss 0.063305\n",
      "Epoch: 36   Loss 0.041428\n",
      "Epoch: 37   Loss 0.024889\n",
      "Epoch: 38   Loss 0.017020\n",
      "Epoch: 39   Loss 0.011790\n",
      "Epoch: 40   Loss 0.008744\n",
      "Epoch: 41   Loss 0.006953\n",
      "Epoch: 42   Loss 0.005808\n",
      "Epoch: 43   Loss 0.005008\n",
      "Epoch: 44   Loss 0.004412\n",
      "Epoch: 45   Loss 0.003945\n",
      "Epoch: 46   Loss 0.003566\n",
      "Epoch: 47   Loss 0.003257\n",
      "Epoch: 48   Loss 0.002996\n",
      "Epoch: 49   Loss 0.002774\n",
      "Epoch: 50   Loss 0.002582\n",
      "Epoch: 51   Loss 0.002413\n",
      "Epoch: 52   Loss 0.002264\n",
      "Epoch: 53   Loss 0.002132\n",
      "Epoch: 54   Loss 0.002014\n",
      "Epoch: 55   Loss 0.001907\n",
      "Epoch: 56   Loss 0.001812\n",
      "Epoch: 57   Loss 0.001724\n",
      "Epoch: 58   Loss 0.001645\n",
      "Epoch: 59   Loss 0.001572\n",
      "Epoch: 60   Loss 0.001505\n",
      "Epoch: 61   Loss 0.001444\n",
      "Epoch: 62   Loss 0.001387\n",
      "Epoch: 63   Loss 0.001334\n",
      "Epoch: 64   Loss 0.001285\n",
      "Epoch: 65   Loss 0.001240\n",
      "Epoch: 66   Loss 0.001197\n",
      "Epoch: 67   Loss 0.001158\n",
      "Epoch: 68   Loss 0.001121\n",
      "Epoch: 69   Loss 0.001086\n",
      "Epoch: 70   Loss 0.001053\n",
      "Epoch: 71   Loss 0.001022\n",
      "Epoch: 72   Loss 0.000993\n",
      "Epoch: 73   Loss 0.000966\n",
      "Epoch: 74   Loss 0.000939\n",
      "Epoch: 75   Loss 0.000914\n",
      "Epoch: 76   Loss 0.000891\n",
      "Epoch: 77   Loss 0.000868\n",
      "Epoch: 78   Loss 0.000847\n",
      "Epoch: 79   Loss 0.000826\n",
      "Epoch: 80   Loss 0.000807\n",
      "Epoch: 81   Loss 0.000788\n",
      "Epoch: 82   Loss 0.000770\n",
      "Epoch: 83   Loss 0.000753\n",
      "Epoch: 84   Loss 0.000737\n",
      "Epoch: 85   Loss 0.000721\n",
      "Epoch: 86   Loss 0.000705\n",
      "Epoch: 87   Loss 0.000691\n",
      "Epoch: 88   Loss 0.000677\n",
      "Epoch: 89   Loss 0.000663\n",
      "Epoch: 90   Loss 0.000650\n",
      "Epoch: 91   Loss 0.000638\n",
      "Epoch: 92   Loss 0.000626\n",
      "Epoch: 93   Loss 0.000614\n",
      "Epoch: 94   Loss 0.000603\n",
      "Epoch: 95   Loss 0.000592\n",
      "Epoch: 96   Loss 0.000581\n",
      "Epoch: 97   Loss 0.000571\n",
      "Epoch: 98   Loss 0.000561\n",
      "Epoch: 99   Loss 0.000552\n"
     ]
    }
   ],
   "source": [
    "model2 = ResNet(nb_residual_blocks = 30, nb_channels = 32,kernel_size = 3, nb_classes = 2)\n",
    "train (model2,train_input,train_target,nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_test_errors2 = compute_nb_errors_CrossEntropy2(model2, test_input, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net 19.80% 198/1000\n"
     ]
    }
   ],
   "source": [
    "print('test error Net {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors2) / test_input.size(0),nb_test_errors2, test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n"
     ]
    }
   ],
   "source": [
    "print(nb_test_errors2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet_aux(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet_aux, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size = 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 2)\n",
    "        self.conv3 = nn.Conv2d(1,32,kernel_size=3)\n",
    "        self.fc1 = nn.Linear(64, 200)\n",
    "        self.fc2 = nn.Linear(200, 50)\n",
    "        self.fc3 = nn.Linear(50, 2)\n",
    "        self.fc4 = nn.Linear(64,128)\n",
    "        self.fc5 = nn.Linear(128,10)\n",
    "         \n",
    "    def forward(self, x):\n",
    "        y = x.view(-1,1,14,14)\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size = 3))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size = 2))\n",
    "        x = F.relu(self.fc1(x.view(-1,64)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        y = F.relu(F.max_pool2d(self.conv3(y), kernel_size = 3))\n",
    "        y = F.relu(F.max_pool2d(self.conv2(y), kernel_size = 2))\n",
    "        y = F.relu(self.fc4(y.view(-1,64)))\n",
    "        y = F.relu(self.fc5(y))\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_aux(model,train_input,train_target,train_classes, criterion = nn.MSELoss(),\n",
    "              optimizer =optim.SGD,batch_size=100,eta = 0.1, nb_epochs = 100) :\n",
    "    \n",
    "    # Define the loss and optimizing algorithm\n",
    "    optimizer = optimizer(model.parameters(), lr = eta)\n",
    "    # Train the model\n",
    "    for e in range(nb_epochs):\n",
    "        running_loss = 0.0\n",
    "        for input_, target1,target2 in (zip(train_input.split(batch_size), train_target.split(batch_size),train_classes.split(batch_size))):\n",
    "            outx,outy = model(input_)\n",
    "            loss1 = criterion(outx, target1)\n",
    "            loss2 = criterion(outy,target2.view(200,-1).squeeze())\n",
    "            loss = loss1 + loss2\n",
    "            running_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('Epoch: {}   Loss {:.6f}'.format(e,running_loss ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aux = LeNet_aux()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0   Loss 29.857390\n",
      "Epoch: 1   Loss 29.540755\n",
      "Epoch: 2   Loss 29.111139\n",
      "Epoch: 3   Loss 28.326009\n",
      "Epoch: 4   Loss 27.435583\n",
      "Epoch: 5   Loss 26.628631\n",
      "Epoch: 6   Loss 26.254275\n",
      "Epoch: 7   Loss 25.626024\n",
      "Epoch: 8   Loss 25.057392\n",
      "Epoch: 9   Loss 24.442166\n",
      "Epoch: 10   Loss 24.305090\n",
      "Epoch: 11   Loss 23.270088\n",
      "Epoch: 12   Loss 23.153136\n",
      "Epoch: 13   Loss 22.386861\n",
      "Epoch: 14   Loss 22.242840\n",
      "Epoch: 15   Loss 22.126298\n",
      "Epoch: 16   Loss 21.631952\n",
      "Epoch: 17   Loss 21.191758\n",
      "Epoch: 18   Loss 21.211083\n",
      "Epoch: 19   Loss 21.452410\n",
      "Epoch: 20   Loss 20.552109\n",
      "Epoch: 21   Loss 20.368007\n",
      "Epoch: 22   Loss 20.753324\n",
      "Epoch: 23   Loss 20.055640\n",
      "Epoch: 24   Loss 19.994173\n",
      "Epoch: 25   Loss 19.836569\n",
      "Epoch: 26   Loss 19.647440\n",
      "Epoch: 27   Loss 19.454367\n",
      "Epoch: 28   Loss 19.438775\n",
      "Epoch: 29   Loss 19.521401\n",
      "Epoch: 30   Loss 19.469287\n",
      "Epoch: 31   Loss 19.046090\n",
      "Epoch: 32   Loss 18.985659\n",
      "Epoch: 33   Loss 19.123031\n",
      "Epoch: 34   Loss 18.671175\n",
      "Epoch: 35   Loss 19.060619\n",
      "Epoch: 36   Loss 18.515720\n",
      "Epoch: 37   Loss 18.638992\n",
      "Epoch: 38   Loss 18.980719\n",
      "Epoch: 39   Loss 18.537865\n",
      "Epoch: 40   Loss 18.696255\n",
      "Epoch: 41   Loss 18.050363\n",
      "Epoch: 42   Loss 19.540817\n",
      "Epoch: 43   Loss 17.802443\n",
      "Epoch: 44   Loss 18.602199\n",
      "Epoch: 45   Loss 19.642846\n",
      "Epoch: 46   Loss 18.040220\n",
      "Epoch: 47   Loss 17.910614\n",
      "Epoch: 48   Loss 17.901593\n",
      "Epoch: 49   Loss 17.506654\n",
      "Epoch: 50   Loss 17.707635\n",
      "Epoch: 51   Loss 17.684053\n",
      "Epoch: 52   Loss 17.790511\n",
      "Epoch: 53   Loss 17.453553\n",
      "Epoch: 54   Loss 17.175310\n",
      "Epoch: 55   Loss 18.180081\n",
      "Epoch: 56   Loss 17.347462\n",
      "Epoch: 57   Loss 17.346521\n",
      "Epoch: 58   Loss 16.961918\n",
      "Epoch: 59   Loss 17.186444\n",
      "Epoch: 60   Loss 17.275432\n",
      "Epoch: 61   Loss 16.752832\n",
      "Epoch: 62   Loss 16.891929\n",
      "Epoch: 63   Loss 17.401558\n",
      "Epoch: 64   Loss 16.356405\n",
      "Epoch: 65   Loss 18.605176\n",
      "Epoch: 66   Loss 16.528354\n",
      "Epoch: 67   Loss 16.759933\n",
      "Epoch: 68   Loss 17.100027\n",
      "Epoch: 69   Loss 16.238670\n",
      "Epoch: 70   Loss 16.921599\n",
      "Epoch: 71   Loss 16.501080\n",
      "Epoch: 72   Loss 16.927133\n",
      "Epoch: 73   Loss 15.749367\n",
      "Epoch: 74   Loss 17.541573\n"
     ]
    }
   ],
   "source": [
    "train_aux(model_aux,train_input,train_target,train_classes,nn.CrossEntropyLoss(),nb_epochs=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_CrossEntropy3(model, data_input, data_target) :\n",
    "    nb_errors,mini_batch_size = 0, 100\n",
    "\n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        outx,_ = model(data_input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = outx.max(1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if data_target[b + k] != predicted_classes[k]:\n",
    "                nb_errors = nb_errors + 1\n",
    "\n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net 18.10% 181/1000\n"
     ]
    }
   ],
   "source": [
    "nb_test_errors3 = compute_nb_errors_CrossEntropy3(model_aux, test_input, test_target)\n",
    "print('test error Net {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors3) / test_input.size(0),nb_test_errors3, test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
