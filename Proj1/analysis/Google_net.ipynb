{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 1\n",
    "import torch \n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from torch import nn \n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from utils.loader import load\n",
    "from utils.loader import PairSetMNIST\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset as a Dataset object\n",
    "train_data = PairSetMNIST(train=True,swap_channel = True)\n",
    "test_data  = PairSetMNIST(test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module) :\n",
    "    \"\"\"\n",
    "    basic 2d convolution with batch norm\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels,out_channels,kernel_size = 1,stride =1, padding = 0) :\n",
    "        super(conv_block,self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels,out_channels,kernel_size,stride ,padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "    \n",
    "    def forward(self,x) :\n",
    "        x = self.bn(self.conv(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Inception block with four different filters scale\n",
    "    \"\"\"\n",
    "    def __init__(self,in_channels,channels_1x1,channels_3x3,channels_5x5,pool_channels):\n",
    "        super(Inception_block, self).__init__()\n",
    "        # 1x1 convolution\n",
    "        self.conv1x1 = conv_block(in_channels,channels_1x1, kernel_size = 1)\n",
    "        # 3x3 convolution factorized in 1x3 followed by 3x1\n",
    "        self.conv3x3 = nn.Sequential(conv_block(in_channels,channels_3x3, kernel_size = 1),\n",
    "                                     conv_block(channels_3x3, channels_3x3, kernel_size = (1,3), padding = (0,1)),\n",
    "                                     conv_block(channels_3x3, channels_3x3, kernel_size = (3,1), padding = (1,0)))\n",
    "        # 5x5 convolution factorized in two consecutive 3x3 implemented as above\n",
    "        self.conv5x5 = nn.Sequential(conv_block(in_channels,channels_5x5, kernel_size = 1),\n",
    "                                     conv_block(channels_5x5, channels_5x5, kernel_size = (1,3),padding =(0,1)),\n",
    "                                     conv_block(channels_5x5, channels_5x5, kernel_size = (3,1), padding = (1,0)),\n",
    "                                     conv_block(channels_5x5,channels_5x5, kernel_size = (1,3),padding=(0,1)),\n",
    "                                     conv_block(channels_5x5, channels_5x5, kernel_size = (3,1),padding = (1,0)))\n",
    "        # pooling layer \n",
    "        self.pool = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=1, padding=1, ceil_mode=True),\n",
    "                                  conv_block(in_channels, pool_channels, kernel_size=1))\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # compute the four filter of the inception block :  Nx64x14x14\n",
    "        scale1 = F.relu(self.conv1x1(x))\n",
    "        scale2 = F.relu(self.conv3x3(x))\n",
    "        scale3 = F.relu(self.conv5x5(x))\n",
    "        scale4 = F.relu(self.pool(x))\n",
    "        \n",
    "        # concatenate layer for next result\n",
    "        outputs = [scale1, scale2, scale3, scale4]\n",
    "        # Nx256x14x14\n",
    "        filter_cat = torch.cat(outputs,1)\n",
    "        \n",
    "        return filter_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Auxiliary_loss (nn.Module) :\n",
    "    \"\"\"\n",
    "    auxiliary loss classification of the digit\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,in_channels,drop_prob_aux,nb_classes = 10):\n",
    "        super(Auxiliary_loss, self).__init__()\n",
    "        \n",
    "        self.conv = conv_block(in_channels, 128, kernel_size=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, nb_classes)\n",
    "        self.dropout= nn.Dropout(drop_prob_aux)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # aux: N x 256 x 14 x 14\n",
    "        x = F.adaptive_avg_pool2d(x, (4, 4))\n",
    "        # aux: N x 256 x 4 x 4\n",
    "        x = self.conv(x)\n",
    "        # N x 128 x 4 x 4\n",
    "        x = x.view(-1,2048)\n",
    "        # N x 2048\n",
    "        x = F.relu(self.fc1(x), inplace=True)\n",
    "        # N x 1024\n",
    "        x = self.dropout(x)\n",
    "        # N x 10 (nb_classes)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Google_Net (nn.Module) :\n",
    "    \"\"\"\n",
    "    Google net implementing two inception layer in parralel for each channel\n",
    "    Use auxiliary loss to classify the digit number\n",
    "    Concatenate the number classification feature map and classify the two channel\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,channels_1x1,channels_3x3,channels_5x5,pool_channels,nhidden = 60,\n",
    "                 drop_prob = 0,drop_prob_aux = 0.7,nb_classes = 10):\n",
    "        super(Google_Net, self).__init__()\n",
    "        \n",
    "        # local response norm\n",
    "        self.conv1 = conv_block(1, 32, kernel_size = 3, padding = (3 - 1)//2)\n",
    "        #inception block\n",
    "        self.inception1 = Inception_block(1,channels_1x1,channels_3x3,channels_5x5,pool_channels)\n",
    "        self.inception2 = Inception_block(256,channels_1x1,channels_3x3,channels_5x5,pool_channels)\n",
    "        #auxiliary\n",
    "        self.auxiliary = Auxiliary_loss(256,drop_prob_aux)\n",
    "        \n",
    "        # weights for binary classification \n",
    "        self.fc1 = nn.Linear(20, nhidden)\n",
    "        self.fc2 = nn.Linear(nhidden, 90)\n",
    "        self.fc3 = nn.Linear(90, 2)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        \n",
    "        # split the 2-channel input into two 1*14*14 images\n",
    "        x = input_[:, 0, :, :].view(-1, 1, 14, 14)\n",
    "        y = input_[:, 1, :, :].view(-1, 1, 14, 14)\n",
    "        \n",
    "        # Local response norm\n",
    "        #x = self.conv1(x)\n",
    "        #y = self.conv1(y)\n",
    "        \n",
    "        # inception blocks\n",
    "        x = self.inception1(x)\n",
    "        y = self.inception1(y)\n",
    "        \n",
    "        \n",
    "        # auxiliary loss \n",
    "        x = self.auxiliary(x)\n",
    "        y = self.auxiliary(y)\n",
    "        \n",
    "        # concatenate layers  \n",
    "        z = torch.cat([x, y], 1)\n",
    "        \n",
    "        z = F.relu(self.fc1(z))\n",
    "        z = F.relu(self.fc2(z))\n",
    "        z = self.dropout(z)\n",
    "        z = self.fc3(z)\n",
    "        \n",
    "        \n",
    "        return x,y,z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### train function ######\n",
    "\n",
    "def train_aux (model, train_data, mini_batch_size=100, optimizer = optim.SGD,\n",
    "                criterion = nn.CrossEntropyLoss(), n_epochs=50, eta=1e-1, lambda_l2 = 0, alpha=0.5, beta=0.5):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Train network with auxiliary loss + weight sharing\n",
    "    \n",
    "    \"\"\"\n",
    "    # create data loader\n",
    "    train_loader = DataLoader(train_data, batch_size=mini_batch_size, shuffle=True)\n",
    "    \n",
    "    model.train()\n",
    "    optimizer = optimizer(model.parameters(), lr = eta)\n",
    "    \n",
    "    for e in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            \n",
    "            input_, target_, classes_ = data\n",
    "            class_1, class_2,out= model(input_)\n",
    "            aux_loss1 = criterion(class_1, classes_[:,0])\n",
    "            aux_loss2 = criterion(class_2, classes_[:,1])\n",
    "            out_loss  = criterion(out, target_)\n",
    "            net_loss = (alpha * (out_loss) + beta * (aux_loss1 + aux_loss2 ))\n",
    "            epoch_loss += net_loss\n",
    "            \n",
    "            if lambda_l2 != 0:\n",
    "                for p in model.parameters():\n",
    "                    epoch_loss += lambda_l2 * p.pow(2).sum() # add an l2 penalty term to the loss \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            net_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print('Train Epoch: {}  | Loss {:.6f}'.format(\n",
    "                e, epoch_loss.item()))\n",
    "        \n",
    "#########################################################################################################################\n",
    "#########################################################################################################################\n",
    "\n",
    "### test function  ###\n",
    "\n",
    "def test_aux(model, test_data, mini_batch_size=100, criterion = nn.CrossEntropyLoss()):\n",
    "    \n",
    "    \"\"\"\n",
    "    Test function to calculate prediction accuracy of a cnn with auxiliary loss\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # create test laoder\n",
    "    test_loader = DataLoader(test_data, batch_size=mini_batch_size, shuffle=True)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    nb_errors=0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            input_, target_, classes_ = data\n",
    "            \n",
    "            _,_ ,output = model(input_) \n",
    "            batch_loss = criterion(output, target_)\n",
    "            test_loss += batch_loss\n",
    "            \n",
    "            _, predicted_classes = output.max(1)\n",
    "            for k in range(mini_batch_size):\n",
    "                if target_[k] != predicted_classes[k]:\n",
    "                    nb_errors = nb_errors + 1\n",
    "                                   \n",
    "             \n",
    "        print('\\nTest set | Loss: {:.4f} | Accuracy: {:.0f}% | # misclassified : {}/{}\\n'.format(\n",
    "        test_loss.item(), 100 * (len(test_data)-nb_errors)/len(test_data), nb_errors, len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0  | Loss 29.767370\n",
      "Train Epoch: 1  | Loss 13.516229\n",
      "Train Epoch: 2  | Loss 8.503138\n",
      "Train Epoch: 3  | Loss 6.766137\n",
      "Train Epoch: 4  | Loss 6.159487\n",
      "Train Epoch: 5  | Loss 4.913076\n",
      "Train Epoch: 6  | Loss 4.067536\n",
      "Train Epoch: 7  | Loss 3.638935\n",
      "Train Epoch: 8  | Loss 3.654038\n",
      "Train Epoch: 9  | Loss 3.028535\n",
      "Train Epoch: 10  | Loss 2.505662\n",
      "Train Epoch: 11  | Loss 2.409002\n",
      "Train Epoch: 12  | Loss 2.404171\n",
      "Train Epoch: 13  | Loss 1.905158\n",
      "Train Epoch: 14  | Loss 1.695484\n",
      "Train Epoch: 15  | Loss 1.579560\n",
      "Train Epoch: 16  | Loss 1.456288\n",
      "Train Epoch: 17  | Loss 1.149295\n",
      "Train Epoch: 18  | Loss 1.258273\n",
      "Train Epoch: 19  | Loss 1.108944\n",
      "Train Epoch: 20  | Loss 0.973030\n",
      "Train Epoch: 21  | Loss 0.992214\n",
      "Train Epoch: 22  | Loss 0.861399\n",
      "Train Epoch: 23  | Loss 1.920845\n",
      "Train Epoch: 24  | Loss 1.003562\n",
      "Train Epoch: 25  | Loss 0.763496\n",
      "Train Epoch: 26  | Loss 0.588748\n",
      "Train Epoch: 27  | Loss 0.613650\n",
      "Train Epoch: 28  | Loss 0.558315\n",
      "Train Epoch: 29  | Loss 0.466551\n",
      "Train Epoch: 30  | Loss 0.461617\n",
      "Train Epoch: 31  | Loss 0.438529\n",
      "Train Epoch: 32  | Loss 0.930642\n",
      "Train Epoch: 33  | Loss 0.490403\n",
      "Train Epoch: 34  | Loss 0.372598\n",
      "Train Epoch: 35  | Loss 0.416742\n",
      "Train Epoch: 36  | Loss 0.379004\n",
      "Train Epoch: 37  | Loss 0.253630\n",
      "Train Epoch: 38  | Loss 0.319039\n",
      "Train Epoch: 39  | Loss 0.314256\n",
      "Train Epoch: 40  | Loss 0.251965\n",
      "Train Epoch: 41  | Loss 0.216844\n",
      "Train Epoch: 42  | Loss 0.217120\n",
      "Train Epoch: 43  | Loss 0.272020\n",
      "Train Epoch: 44  | Loss 0.253974\n",
      "Train Epoch: 45  | Loss 0.186547\n",
      "Train Epoch: 46  | Loss 0.185143\n",
      "Train Epoch: 47  | Loss 0.220699\n",
      "Train Epoch: 48  | Loss 0.168115\n",
      "Train Epoch: 49  | Loss 0.221224\n",
      "\n",
      "Test set | Loss: 1.4422 | Accuracy: 96% | # misclassified : 37/1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model1 = Google_Net()\n",
    "train_aux(model1, train_data)\n",
    "test_aux(model1,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0  | Loss 32.350616\n",
      "Train Epoch: 1  | Loss 13.768975\n",
      "Train Epoch: 2  | Loss 9.004088\n",
      "Train Epoch: 3  | Loss 6.802941\n",
      "Train Epoch: 4  | Loss 5.937626\n",
      "Train Epoch: 5  | Loss 5.061621\n",
      "Train Epoch: 6  | Loss 4.577953\n",
      "Train Epoch: 7  | Loss 4.060737\n",
      "Train Epoch: 8  | Loss 3.481941\n",
      "Train Epoch: 9  | Loss 3.141851\n",
      "Train Epoch: 10  | Loss 2.864805\n",
      "Train Epoch: 11  | Loss 2.238279\n",
      "Train Epoch: 12  | Loss 2.085346\n",
      "Train Epoch: 13  | Loss 1.725571\n",
      "Train Epoch: 14  | Loss 1.535687\n",
      "Train Epoch: 15  | Loss 1.645390\n",
      "Train Epoch: 16  | Loss 1.230233\n",
      "Train Epoch: 17  | Loss 1.921638\n",
      "Train Epoch: 18  | Loss 1.524699\n",
      "Train Epoch: 19  | Loss 1.127218\n",
      "Train Epoch: 20  | Loss 0.960918\n",
      "Train Epoch: 21  | Loss 0.884457\n",
      "Train Epoch: 22  | Loss 0.779605\n",
      "Train Epoch: 23  | Loss 0.568249\n",
      "Train Epoch: 24  | Loss 0.629181\n",
      "Train Epoch: 25  | Loss 0.582528\n",
      "Train Epoch: 26  | Loss 0.581650\n",
      "Train Epoch: 27  | Loss 0.557396\n",
      "Train Epoch: 28  | Loss 0.516551\n",
      "Train Epoch: 29  | Loss 0.561240\n",
      "Train Epoch: 30  | Loss 0.454696\n",
      "Train Epoch: 31  | Loss 0.579024\n",
      "Train Epoch: 32  | Loss 0.444872\n",
      "Train Epoch: 33  | Loss 0.341385\n",
      "Train Epoch: 34  | Loss 0.304100\n",
      "Train Epoch: 35  | Loss 0.373499\n",
      "Train Epoch: 36  | Loss 0.280900\n",
      "Train Epoch: 37  | Loss 0.402086\n",
      "Train Epoch: 38  | Loss 0.282460\n",
      "Train Epoch: 39  | Loss 0.188300\n",
      "Train Epoch: 40  | Loss 0.209868\n",
      "Train Epoch: 41  | Loss 0.261395\n",
      "Train Epoch: 42  | Loss 0.272252\n",
      "Train Epoch: 43  | Loss 0.214421\n",
      "Train Epoch: 44  | Loss 0.207925\n",
      "Train Epoch: 45  | Loss 0.203880\n",
      "Train Epoch: 46  | Loss 0.225202\n",
      "Train Epoch: 47  | Loss 0.149295\n",
      "Train Epoch: 48  | Loss 0.164753\n",
      "Train Epoch: 49  | Loss 0.198603\n",
      "\n",
      "Test set | Loss: 1.5976 | Accuracy: 96% | # misclassified : 45/1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2 = Google_Net(64,64,64,64)\n",
    "train_aux(model2, train_data)\n",
    "test_aux(model2,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0  | Loss 32.565662\n",
      "Train Epoch: 1  | Loss 14.515175\n",
      "Train Epoch: 2  | Loss 8.981249\n",
      "Train Epoch: 3  | Loss 6.971626\n",
      "Train Epoch: 4  | Loss 6.269598\n",
      "Train Epoch: 5  | Loss 5.010170\n",
      "Train Epoch: 6  | Loss 4.414513\n",
      "Train Epoch: 7  | Loss 3.765643\n",
      "Train Epoch: 8  | Loss 3.314440\n",
      "Train Epoch: 9  | Loss 3.376633\n",
      "Train Epoch: 10  | Loss 2.951209\n",
      "Train Epoch: 11  | Loss 2.482831\n",
      "Train Epoch: 12  | Loss 2.482694\n",
      "Train Epoch: 13  | Loss 2.026098\n",
      "Train Epoch: 14  | Loss 1.793649\n",
      "Train Epoch: 15  | Loss 1.911194\n",
      "Train Epoch: 16  | Loss 1.520952\n",
      "Train Epoch: 17  | Loss 1.366843\n",
      "Train Epoch: 18  | Loss 1.324510\n",
      "Train Epoch: 19  | Loss 1.021954\n",
      "Train Epoch: 20  | Loss 1.113924\n",
      "Train Epoch: 21  | Loss 0.861108\n",
      "Train Epoch: 22  | Loss 0.904173\n",
      "Train Epoch: 23  | Loss 0.887597\n",
      "Train Epoch: 24  | Loss 0.901606\n",
      "Train Epoch: 25  | Loss 0.819775\n",
      "Train Epoch: 26  | Loss 0.671401\n",
      "Train Epoch: 27  | Loss 0.491444\n",
      "Train Epoch: 28  | Loss 0.651164\n",
      "Train Epoch: 29  | Loss 0.532023\n",
      "Train Epoch: 30  | Loss 0.468494\n",
      "Train Epoch: 31  | Loss 0.497019\n",
      "Train Epoch: 32  | Loss 0.926379\n",
      "Train Epoch: 33  | Loss 0.404400\n",
      "Train Epoch: 34  | Loss 0.433764\n",
      "Train Epoch: 35  | Loss 0.381710\n",
      "Train Epoch: 36  | Loss 0.272585\n",
      "Train Epoch: 37  | Loss 0.280991\n",
      "Train Epoch: 38  | Loss 0.322502\n",
      "Train Epoch: 39  | Loss 0.309523\n",
      "Train Epoch: 40  | Loss 0.265566\n",
      "Train Epoch: 41  | Loss 0.258056\n",
      "Train Epoch: 42  | Loss 0.202841\n",
      "Train Epoch: 43  | Loss 0.205950\n",
      "Train Epoch: 44  | Loss 0.213031\n",
      "Train Epoch: 45  | Loss 0.319324\n",
      "Train Epoch: 46  | Loss 0.247877\n",
      "Train Epoch: 47  | Loss 0.203272\n",
      "Train Epoch: 48  | Loss 0.172364\n",
      "Train Epoch: 49  | Loss 0.131102\n",
      "\n",
      "Test set | Loss: 1.7791 | Accuracy: 95% | # misclassified : 47/1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model3 = Google_Net(64,64,64,64)\n",
    "train_aux(model3, train_data)\n",
    "test_aux(model3,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0  | Loss 31.853571\n",
      "Train Epoch: 1  | Loss 13.978946\n",
      "Train Epoch: 2  | Loss 9.044988\n",
      "Train Epoch: 3  | Loss 7.276228\n",
      "Train Epoch: 4  | Loss 5.823573\n",
      "Train Epoch: 5  | Loss 4.920546\n",
      "Train Epoch: 6  | Loss 4.541502\n",
      "Train Epoch: 7  | Loss 3.792336\n",
      "Train Epoch: 8  | Loss 3.247078\n",
      "Train Epoch: 9  | Loss 2.945832\n",
      "Train Epoch: 10  | Loss 2.867447\n",
      "Train Epoch: 11  | Loss 2.398836\n",
      "Train Epoch: 12  | Loss 2.897313\n",
      "Train Epoch: 13  | Loss 1.970138\n",
      "Train Epoch: 14  | Loss 1.753320\n",
      "Train Epoch: 15  | Loss 1.540551\n",
      "Train Epoch: 16  | Loss 1.412518\n",
      "Train Epoch: 17  | Loss 1.375399\n",
      "Train Epoch: 18  | Loss 1.086837\n",
      "Train Epoch: 19  | Loss 1.367363\n",
      "Train Epoch: 20  | Loss 1.236998\n",
      "Train Epoch: 21  | Loss 0.909724\n",
      "Train Epoch: 22  | Loss 0.870065\n",
      "Train Epoch: 23  | Loss 0.714281\n",
      "Train Epoch: 24  | Loss 0.648039\n",
      "Train Epoch: 25  | Loss 0.646123\n",
      "Train Epoch: 26  | Loss 0.593228\n",
      "Train Epoch: 27  | Loss 0.461571\n",
      "Train Epoch: 28  | Loss 0.518458\n",
      "Train Epoch: 29  | Loss 0.440357\n",
      "Train Epoch: 30  | Loss 0.352848\n",
      "Train Epoch: 31  | Loss 0.405679\n",
      "Train Epoch: 32  | Loss 0.364307\n",
      "Train Epoch: 33  | Loss 0.309583\n",
      "Train Epoch: 34  | Loss 0.363997\n",
      "Train Epoch: 35  | Loss 0.286669\n",
      "Train Epoch: 36  | Loss 0.266727\n",
      "Train Epoch: 37  | Loss 0.304905\n",
      "Train Epoch: 38  | Loss 0.247216\n",
      "Train Epoch: 39  | Loss 0.224213\n",
      "Train Epoch: 40  | Loss 0.183776\n",
      "Train Epoch: 41  | Loss 0.232765\n",
      "Train Epoch: 42  | Loss 0.204053\n",
      "Train Epoch: 43  | Loss 0.164573\n",
      "Train Epoch: 44  | Loss 0.230667\n",
      "Train Epoch: 45  | Loss 0.165294\n",
      "Train Epoch: 46  | Loss 0.189155\n",
      "Train Epoch: 47  | Loss 0.213672\n",
      "Train Epoch: 48  | Loss 0.130824\n",
      "Train Epoch: 49  | Loss 0.209595\n",
      "\n",
      "Test set | Loss: 1.6016 | Accuracy: 96% | # misclassified : 43/1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model4 = Google_Net(64,64,64,64,drop_prob = 0.5)\n",
    "train_aux(model4, train_data)\n",
    "test_aux(model4,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0  | Loss 26.392162\n",
      "Train Epoch: 1  | Loss 10.809406\n",
      "Train Epoch: 2  | Loss 7.564365\n",
      "Train Epoch: 3  | Loss 6.040642\n",
      "Train Epoch: 4  | Loss 4.994551\n",
      "Train Epoch: 5  | Loss 4.132074\n",
      "Train Epoch: 6  | Loss 3.559065\n",
      "Train Epoch: 7  | Loss 3.158071\n",
      "Train Epoch: 8  | Loss 2.641409\n",
      "Train Epoch: 9  | Loss 2.734702\n",
      "Train Epoch: 10  | Loss 2.332850\n",
      "Train Epoch: 11  | Loss 1.981328\n",
      "Train Epoch: 12  | Loss 1.947118\n",
      "Train Epoch: 13  | Loss 1.671224\n",
      "Train Epoch: 14  | Loss 1.351950\n",
      "Train Epoch: 15  | Loss 1.461564\n",
      "Train Epoch: 16  | Loss 1.429318\n",
      "Train Epoch: 17  | Loss 1.179722\n",
      "Train Epoch: 18  | Loss 0.950029\n",
      "Train Epoch: 19  | Loss 1.024328\n",
      "Train Epoch: 20  | Loss 0.803164\n",
      "Train Epoch: 21  | Loss 0.799966\n",
      "Train Epoch: 22  | Loss 0.673487\n",
      "Train Epoch: 23  | Loss 0.654501\n",
      "Train Epoch: 24  | Loss 0.514970\n",
      "Train Epoch: 25  | Loss 0.601667\n",
      "Train Epoch: 26  | Loss 0.549156\n",
      "Train Epoch: 27  | Loss 0.524546\n",
      "Train Epoch: 28  | Loss 0.437109\n",
      "Train Epoch: 29  | Loss 0.412676\n",
      "Train Epoch: 30  | Loss 0.426071\n",
      "Train Epoch: 31  | Loss 0.491957\n",
      "Train Epoch: 32  | Loss 0.419111\n",
      "Train Epoch: 33  | Loss 0.363630\n",
      "Train Epoch: 34  | Loss 0.433187\n",
      "Train Epoch: 35  | Loss 0.520670\n",
      "Train Epoch: 36  | Loss 0.278123\n",
      "Train Epoch: 37  | Loss 0.342681\n",
      "Train Epoch: 38  | Loss 0.411355\n",
      "Train Epoch: 39  | Loss 0.306080\n",
      "Train Epoch: 40  | Loss 0.244018\n",
      "Train Epoch: 41  | Loss 0.263553\n",
      "Train Epoch: 42  | Loss 0.243615\n",
      "Train Epoch: 43  | Loss 0.139655\n",
      "Train Epoch: 44  | Loss 0.308589\n",
      "Train Epoch: 45  | Loss 0.217186\n",
      "Train Epoch: 46  | Loss 0.214793\n",
      "Train Epoch: 47  | Loss 0.235104\n",
      "Train Epoch: 48  | Loss 0.146651\n",
      "Train Epoch: 49  | Loss 0.306830\n",
      "\n",
      "Test set | Loss: 2.0192 | Accuracy: 95% | # misclassified : 48/1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model5 = Google_Net(64,64,64,64,drop_prob = 0.5)\n",
    "train_aux(model5, train_data)\n",
    "test_aux(model5,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0  | Loss 36.097652\n",
      "Train Epoch: 1  | Loss 12.594378\n",
      "Train Epoch: 2  | Loss 7.989007\n",
      "Train Epoch: 3  | Loss 5.971697\n",
      "Train Epoch: 4  | Loss 5.103000\n",
      "Train Epoch: 5  | Loss 4.165145\n",
      "Train Epoch: 6  | Loss 3.976546\n",
      "Train Epoch: 7  | Loss 3.224481\n",
      "Train Epoch: 8  | Loss 2.712321\n",
      "Train Epoch: 9  | Loss 2.628671\n",
      "Train Epoch: 10  | Loss 2.293429\n",
      "Train Epoch: 11  | Loss 1.924949\n",
      "Train Epoch: 12  | Loss 1.738328\n",
      "Train Epoch: 13  | Loss 1.711177\n",
      "Train Epoch: 14  | Loss 1.793992\n",
      "Train Epoch: 15  | Loss 1.166682\n",
      "Train Epoch: 16  | Loss 1.112388\n",
      "Train Epoch: 17  | Loss 1.252998\n",
      "Train Epoch: 18  | Loss 1.083019\n",
      "Train Epoch: 19  | Loss 0.888970\n",
      "Train Epoch: 20  | Loss 0.879558\n",
      "Train Epoch: 21  | Loss 0.714483\n",
      "Train Epoch: 22  | Loss 0.639415\n",
      "Train Epoch: 23  | Loss 1.050229\n",
      "Train Epoch: 24  | Loss 0.612210\n",
      "Train Epoch: 25  | Loss 0.574584\n",
      "Train Epoch: 26  | Loss 0.454151\n",
      "Train Epoch: 27  | Loss 0.385831\n",
      "Train Epoch: 28  | Loss 0.484206\n",
      "Train Epoch: 29  | Loss 0.379391\n",
      "Train Epoch: 30  | Loss 0.257406\n",
      "Train Epoch: 31  | Loss 0.484254\n",
      "Train Epoch: 32  | Loss 0.492502\n",
      "Train Epoch: 33  | Loss 0.531153\n",
      "Train Epoch: 34  | Loss 0.286871\n",
      "Train Epoch: 35  | Loss 0.422247\n",
      "Train Epoch: 36  | Loss 0.277323\n",
      "Train Epoch: 37  | Loss 0.315836\n",
      "Train Epoch: 38  | Loss 0.273055\n",
      "Train Epoch: 39  | Loss 0.222578\n",
      "Train Epoch: 40  | Loss 0.375646\n",
      "Train Epoch: 41  | Loss 0.411220\n",
      "Train Epoch: 42  | Loss 0.195671\n",
      "Train Epoch: 43  | Loss 0.133220\n",
      "Train Epoch: 44  | Loss 0.455927\n",
      "Train Epoch: 45  | Loss 0.258843\n",
      "Train Epoch: 46  | Loss 0.140770\n",
      "Train Epoch: 47  | Loss 0.115532\n",
      "Train Epoch: 48  | Loss 0.123228\n",
      "Train Epoch: 49  | Loss 0.257795\n",
      "\n",
      "Test set | Loss: 1.7170 | Accuracy: 95% | # misclassified : 52/1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model6 = Google_Net(64,64,64,64,drop_prob = 0.5)\n",
    "train_aux(model6, train_data)\n",
    "test_aux(model6,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0  | Loss 23.261400\n",
      "Train Epoch: 1  | Loss 9.118961\n",
      "Train Epoch: 2  | Loss 6.285275\n",
      "Train Epoch: 3  | Loss 4.901327\n",
      "Train Epoch: 4  | Loss 4.493050\n",
      "Train Epoch: 5  | Loss 3.991235\n",
      "Train Epoch: 6  | Loss 3.349438\n",
      "Train Epoch: 7  | Loss 3.045502\n",
      "Train Epoch: 8  | Loss 2.766802\n",
      "Train Epoch: 9  | Loss 2.464422\n",
      "Train Epoch: 10  | Loss 2.135505\n",
      "Train Epoch: 11  | Loss 1.712572\n",
      "Train Epoch: 12  | Loss 1.663981\n",
      "Train Epoch: 13  | Loss 1.683771\n",
      "Train Epoch: 14  | Loss 1.349212\n",
      "Train Epoch: 15  | Loss 1.171406\n",
      "Train Epoch: 16  | Loss 1.174358\n",
      "Train Epoch: 17  | Loss 1.172775\n",
      "Train Epoch: 18  | Loss 1.143875\n",
      "Train Epoch: 19  | Loss 1.106894\n",
      "Train Epoch: 20  | Loss 1.042744\n",
      "Train Epoch: 21  | Loss 1.001812\n",
      "Train Epoch: 22  | Loss 0.845421\n",
      "Train Epoch: 23  | Loss 0.783974\n",
      "Train Epoch: 24  | Loss 0.974004\n",
      "Train Epoch: 25  | Loss 0.941843\n",
      "Train Epoch: 26  | Loss 0.698549\n",
      "Train Epoch: 27  | Loss 1.214441\n",
      "Train Epoch: 28  | Loss 0.815623\n",
      "Train Epoch: 29  | Loss 0.601879\n",
      "Train Epoch: 30  | Loss 0.574249\n",
      "Train Epoch: 31  | Loss 0.534387\n",
      "Train Epoch: 32  | Loss 0.929526\n",
      "Train Epoch: 33  | Loss 0.904203\n",
      "Train Epoch: 34  | Loss 0.898937\n",
      "Train Epoch: 35  | Loss 0.905861\n",
      "Train Epoch: 36  | Loss 0.790641\n",
      "Train Epoch: 37  | Loss 0.836256\n",
      "Train Epoch: 38  | Loss 0.892449\n",
      "Train Epoch: 39  | Loss 0.985794\n",
      "Train Epoch: 40  | Loss 0.724113\n",
      "Train Epoch: 41  | Loss 0.631118\n",
      "Train Epoch: 42  | Loss 0.829521\n",
      "Train Epoch: 43  | Loss 0.976482\n",
      "Train Epoch: 44  | Loss 0.598369\n",
      "Train Epoch: 45  | Loss 0.491792\n",
      "Train Epoch: 46  | Loss 0.394108\n",
      "Train Epoch: 47  | Loss 0.477884\n",
      "Train Epoch: 48  | Loss 0.615528\n",
      "Train Epoch: 49  | Loss 0.594125\n",
      "\n",
      "Test set | Loss: 1.5452 | Accuracy: 96% | # misclassified : 39/1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model6 = Google_Net(64,64,64,64,drop_prob = 0.5)\n",
    "train_aux(model6, train_data, optimizer = optim.Adam, eta = 0.001)\n",
    "test_aux(model6,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
