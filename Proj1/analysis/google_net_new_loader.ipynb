{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from torch import nn \n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from utils.new_loader import load,PairSetMNIST,Training_set,Test_set, Training_set_split,Validation_set\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8000, 2, 14, 14])\n",
      "torch.Size([1000, 2, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "# load the dataset as a Dataset object\n",
    "data = PairSetMNIST( rotate=True,translate=True,swap_channel = False)\n",
    "train_data = Training_set(data)\n",
    "test_data = Test_set(data)\n",
    "print(train_data.train_input.shape)\n",
    "print(test_data.test_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module) :\n",
    "    \"\"\"\n",
    "    basic 2d convolution with batch norm\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels,out_channels,kernel_size = 1,stride =1, padding = 0) :\n",
    "        super(conv_block,self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels,out_channels,kernel_size,stride ,padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "    \n",
    "    def forward(self,x) :\n",
    "        x = self.bn(self.conv(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Inception block with four different filters scale\n",
    "    \"\"\"\n",
    "    def __init__(self,in_channels,channels_1x1,channels_3x3,channels_5x5,pool_channels):\n",
    "        super(Inception_block, self).__init__()\n",
    "        # 1x1 convolution\n",
    "        self.conv1x1 = conv_block(in_channels,channels_1x1, kernel_size = 1)\n",
    "        # 3x3 convolution factorized in 1x3 followed by 3x1\n",
    "        self.conv3x3 = nn.Sequential(conv_block(in_channels,channels_3x3, kernel_size = 1),\n",
    "                                     conv_block(channels_3x3, channels_3x3, kernel_size = (1,3), padding = (0,1)),\n",
    "                                     conv_block(channels_3x3, channels_3x3, kernel_size = (3,1), padding = (1,0)))\n",
    "        # 5x5 convolution factorized in two consecutive 3x3 implemented as above\n",
    "        self.conv5x5 = nn.Sequential(conv_block(in_channels,channels_5x5, kernel_size = 1),\n",
    "                                     conv_block(channels_5x5, channels_5x5, kernel_size = (1,3),padding =(0,1)),\n",
    "                                     conv_block(channels_5x5, channels_5x5, kernel_size = (3,1), padding = (1,0)),\n",
    "                                     conv_block(channels_5x5,channels_5x5, kernel_size = (1,3),padding=(0,1)),\n",
    "                                     conv_block(channels_5x5, channels_5x5, kernel_size = (3,1),padding = (1,0)))\n",
    "        # pooling layer \n",
    "        self.pool = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                                  conv_block(in_channels, pool_channels, kernel_size=1))\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # compute the four filter of the inception block :  Nx64x14x14\n",
    "        scale1 = F.relu(self.conv1x1(x))\n",
    "        scale2 = F.relu(self.conv3x3(x))\n",
    "        scale3 = F.relu(self.conv5x5(x))\n",
    "        scale4 = F.relu(self.pool(x))\n",
    "        \n",
    "        # concatenate layer for next result\n",
    "        outputs = [scale1, scale2, scale3, scale4]\n",
    "        # Nx256x14x14\n",
    "        filter_cat = torch.cat(outputs,1)\n",
    "        \n",
    "        return filter_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Auxiliary_loss (nn.Module) :\n",
    "    \"\"\"\n",
    "    auxiliary loss classification of the digit\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,in_channels,drop_prob_aux,nb_classes = 10):\n",
    "        super(Auxiliary_loss, self).__init__()\n",
    "        \n",
    "        self.conv = conv_block(in_channels, 128, kernel_size=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, nb_classes)\n",
    "        self.dropout= nn.Dropout(drop_prob_aux)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # aux: N x 256 x 14 x 14\n",
    "        x = F.adaptive_avg_pool2d(x, (4, 4))\n",
    "        # aux: N x 256 x 4 x 4\n",
    "        x = self.conv(x)\n",
    "        # N x 128 x 4 x 4\n",
    "        x = x.view(-1,2048)\n",
    "        # N x 2048\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # N x 1024\n",
    "        x = self.dropout(x)\n",
    "        # N x 10 (nb_classes)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Google_Net (nn.Module) :\n",
    "    \"\"\"\n",
    "    Google net implementing two inception layer in parralel for each channel\n",
    "    Use auxiliary loss to classify the digit number\n",
    "    Concatenate the number classification feature map and classify the two channel\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,channels_1x1,channels_3x3,channels_5x5,pool_channels,nhidden = 60,\n",
    "                 drop_prob = 0,drop_prob_aux = 0.7,nb_classes = 10):\n",
    "        super(Google_Net, self).__init__()\n",
    "        \n",
    "        # local response norm\n",
    "        #self.conv1 = conv_block(1, 32, kernel_size = 3, padding = (3 - 1)//2)\n",
    "        #inception block\n",
    "        self.inception = Inception_block(1,channels_1x1,channels_3x3,channels_5x5,pool_channels)\n",
    "        #auxiliary\n",
    "        self.auxiliary = Auxiliary_loss(256,drop_prob_aux)\n",
    "        \n",
    "        # weights for binary classification \n",
    "        self.fc1 = nn.Linear(20, nhidden)\n",
    "        self.fc2 = nn.Linear(nhidden, 90)\n",
    "        self.fc3 = nn.Linear(90, 2)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        \n",
    "        # split the 2-channel input into two 1*14*14 images\n",
    "        x = input_[:, 0, :, :].view(-1, 1, 14, 14)\n",
    "        y = input_[:, 1, :, :].view(-1, 1, 14, 14)\n",
    "        \n",
    "        # inception blocks\n",
    "        x = self.inception(x)\n",
    "        y = self.inception(y)\n",
    "        \n",
    "        \n",
    "        # auxiliary loss \n",
    "        x = self.auxiliary(x)\n",
    "        y = self.auxiliary(y)\n",
    "        \n",
    "        # concatenate layers  \n",
    "        z = torch.cat([x, y], 1)\n",
    "        \n",
    "        z = F.relu(self.fc1(z))\n",
    "        z = F.relu(self.fc2(z))\n",
    "        z = self.fc3(z)\n",
    "        \n",
    "        \n",
    "        return x,y,z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### train function ######\n",
    "\n",
    "def train_aux (model, train_data, mini_batch_size=100, optimizer = optim.SGD,\n",
    "                criterion = nn.CrossEntropyLoss(), n_epochs=50, eta=1e-1, lambda_l2 = 0, alpha=0.5, beta=0.5):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Train network with auxiliary loss + weight sharing\n",
    "    \n",
    "    \"\"\"\n",
    "    # create data loader\n",
    "    train_loader = DataLoader(train_data, batch_size=mini_batch_size, shuffle=True)\n",
    "    \n",
    "    model.train()\n",
    "    optimizer = optimizer(model.parameters(), lr = eta)\n",
    "    \n",
    "    for e in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            \n",
    "            input_, target_, classes_ = data\n",
    "            class_1, class_2,out= model(input_)\n",
    "            aux_loss1 = criterion(class_1, classes_[:,0])\n",
    "            aux_loss2 = criterion(class_2, classes_[:,1])\n",
    "            out_loss  = criterion(out, target_)\n",
    "            net_loss = (alpha * (out_loss) + beta * (aux_loss1 + aux_loss2 ))\n",
    "            epoch_loss += net_loss\n",
    "            \n",
    "            if lambda_l2 != 0:\n",
    "                for p in model.parameters():\n",
    "                    epoch_loss += lambda_l2 * p.pow(2).sum() # add an l2 penalty term to the loss \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            net_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print('Train Epoch: {}  | Loss {:.6f}'.format(\n",
    "                e, epoch_loss.item()))\n",
    "        \n",
    "#########################################################################################################################\n",
    "#########################################################################################################################\n",
    "\n",
    "### test function  ###\n",
    "\n",
    "def test_aux(model, test_data, mini_batch_size=100, criterion = nn.CrossEntropyLoss()):\n",
    "    \n",
    "    \"\"\"\n",
    "    Test function to calculate prediction accuracy of a cnn with auxiliary loss\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # create test laoder\n",
    "    test_loader = DataLoader(test_data, batch_size=mini_batch_size, shuffle=True)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    nb_errors=0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            input_, target_, classes_ = data\n",
    "            \n",
    "            _,_ ,output = model(input_) \n",
    "            batch_loss = criterion(output, target_)\n",
    "            test_loss += batch_loss\n",
    "            \n",
    "            _, predicted_classes = output.max(1)\n",
    "            for k in range(mini_batch_size):\n",
    "                if target_[k] != predicted_classes[k]:\n",
    "                    nb_errors = nb_errors + 1\n",
    "                                   \n",
    "             \n",
    "        print('\\nTest set | Loss: {:.4f} | Accuracy: {:.0f}% | # misclassified : {}/{}\\n'.format(\n",
    "        test_loss.item(), 100 * (len(test_data)-nb_errors)/len(test_data), nb_errors, len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0  | Loss 103.996536\n",
      "Train Epoch: 1  | Loss 46.128143\n",
      "Train Epoch: 2  | Loss 33.827015\n",
      "Train Epoch: 3  | Loss 27.975136\n",
      "Train Epoch: 4  | Loss 22.793413\n",
      "Train Epoch: 5  | Loss 20.146053\n",
      "Train Epoch: 6  | Loss 18.088533\n",
      "Train Epoch: 7  | Loss 15.358232\n",
      "Train Epoch: 8  | Loss 13.669442\n",
      "Train Epoch: 9  | Loss 12.344196\n",
      "Train Epoch: 10  | Loss 10.887296\n",
      "Train Epoch: 11  | Loss 11.309255\n",
      "Train Epoch: 12  | Loss 9.629958\n",
      "Train Epoch: 13  | Loss 9.117780\n",
      "Train Epoch: 14  | Loss 8.297460\n",
      "Train Epoch: 15  | Loss 8.113937\n",
      "Train Epoch: 16  | Loss 7.084449\n",
      "Train Epoch: 17  | Loss 6.966116\n",
      "Train Epoch: 18  | Loss 6.488020\n",
      "Train Epoch: 19  | Loss 5.918427\n",
      "Train Epoch: 20  | Loss 5.676183\n",
      "Train Epoch: 21  | Loss 5.556067\n",
      "Train Epoch: 22  | Loss 5.066261\n",
      "Train Epoch: 23  | Loss 4.582804\n",
      "Train Epoch: 24  | Loss 4.240164\n",
      "Train Epoch: 25  | Loss 4.252727\n",
      "Train Epoch: 26  | Loss 4.070949\n",
      "Train Epoch: 27  | Loss 4.246456\n",
      "Train Epoch: 28  | Loss 3.410452\n",
      "Train Epoch: 29  | Loss 3.847082\n",
      "Train Epoch: 30  | Loss 4.251037\n",
      "Train Epoch: 31  | Loss 3.353229\n",
      "Train Epoch: 32  | Loss 3.235929\n",
      "Train Epoch: 33  | Loss 3.011529\n",
      "Train Epoch: 34  | Loss 3.061549\n",
      "Train Epoch: 35  | Loss 2.816404\n",
      "Train Epoch: 36  | Loss 2.954069\n",
      "Train Epoch: 37  | Loss 2.430742\n",
      "Train Epoch: 38  | Loss 2.543486\n",
      "Train Epoch: 39  | Loss 2.663092\n",
      "Train Epoch: 40  | Loss 2.216065\n",
      "Train Epoch: 41  | Loss 2.448705\n",
      "Train Epoch: 42  | Loss 2.282431\n",
      "Train Epoch: 43  | Loss 1.954139\n",
      "Train Epoch: 44  | Loss 2.057131\n",
      "Train Epoch: 45  | Loss 1.993379\n",
      "Train Epoch: 46  | Loss 2.095642\n",
      "Train Epoch: 47  | Loss 1.977481\n",
      "Train Epoch: 48  | Loss 1.974764\n",
      "Train Epoch: 49  | Loss 1.969313\n",
      "\n",
      "Test set | Loss: 1.0105 | Accuracy: 97% | # misclassified : 28/1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Google_Net(64,64,64,64)\n",
    "train_aux(model, train_data)\n",
    "test_aux(model,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set | Loss: 1.0740 | Accuracy: 97% | # misclassified : 34/1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_aux(model,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set | Loss: 1.7362 | Accuracy: 96% | # misclassified : 36/1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_aux(model,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set | Loss: 1.4627 | Accuracy: 97% | # misclassified : 33/1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_aux(model,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
