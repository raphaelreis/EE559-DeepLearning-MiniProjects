{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 1\n",
    "import torch \n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from torch import nn \n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from utils.loader import load\n",
    "from utils.loader import PairSetMNIST\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset as a Dataset object\n",
    "train_data = PairSetMNIST(train=True,swap_channel = True)\n",
    "test_data  = PairSetMNIST(test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STN_Net_aux(nn.Module):\n",
    "    \"\"\"\n",
    "    Weight sharing + Auxiliary loss + spatial transformer\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(STN_Net_aux, self).__init__()\n",
    "        # convolutional weights for digit reocgnition shared for each image\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "        \n",
    "        # weights for binary classification \n",
    "        self.fc3 = nn.Linear(20, 230)\n",
    "        self.fc4 = nn.Linear(230, 100)\n",
    "        self.fc5 = nn.Linear(100, 2)\n",
    "        \n",
    "        # Spatial transformer localization-network\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=3),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(10 * 2 * 2, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "        \n",
    "    # Spatial transformer network forward function\n",
    "    def stn(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 10 * 2 * 2)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "\n",
    "        return x\n",
    "        \n",
    "    def forward(self, input_):    \n",
    "        \n",
    "        # split the 2-channel input into two 14*14 images\n",
    "        x = input_[:, 0, :, :].view(-1, 1, 14, 14)\n",
    "        y = input_[:, 1, :, :].view(-1, 1, 14, 14)\n",
    "        \n",
    "        # spatial transformer network\n",
    "        x_ = self.stn(x)\n",
    "        y_ =self.stn(y)\n",
    "        \n",
    "        \n",
    "        # forward pass for the first image \n",
    "        x = F.relu(F.max_pool2d(self.bn1(self.conv1(x)), kernel_size=2, stride=2))\n",
    "        x = F.relu(F.max_pool2d(self.bn2(self.conv2(x)), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.dropout(self.fc1(x.view(-1, 256))))\n",
    "        x = self.dropout(self.fc2(x))\n",
    "        \n",
    "        # forward pass for the second image \n",
    "        y = F.relu(F.max_pool2d(self.bn1(self.conv1(y)), kernel_size=2, stride=2))\n",
    "        y = F.relu(F.max_pool2d(self.bn2(self.conv2(y)), kernel_size=2, stride=2))\n",
    "        y = F.relu(self.dropout(self.fc1(y.view(-1, 256))))\n",
    "        y = self.dropout(self.fc2(y))\n",
    "        \n",
    "        # forward pass for the first image transformed\n",
    "        x_ = F.relu(F.max_pool2d(self.bn1(self.conv1(x_)), kernel_size=2, stride=2))\n",
    "        x_ = F.relu(F.max_pool2d(self.bn2(self.conv2(x_)), kernel_size=2, stride=2))\n",
    "        x_ = F.relu(self.dropout(self.fc1(x_.view(-1, 256))))\n",
    "        x_ = self.dropout(self.fc2(x_))\n",
    "        \n",
    "        # forward pass for the first image transformed\n",
    "        y_ = F.relu(F.max_pool2d(self.bn1(self.conv1(y_)), kernel_size=2, stride=2))\n",
    "        y_ = F.relu(F.max_pool2d(self.bn2(self.conv2(y_)), kernel_size=2, stride=2))\n",
    "        y_ = F.relu(self.dropout(self.fc1(y_.view(-1, 256))))\n",
    "        y_ = self.dropout(self.fc2(y_))\n",
    "        \n",
    "        # concatenate layers  \n",
    "        z = torch.cat([x, y], 1)\n",
    "        \n",
    "        # concatenate layers images transformed \n",
    "        z_ = torch.cat([x_, y_], 1)\n",
    "        \n",
    "        z = F.relu(self.dropout(self.fc3(z)))\n",
    "        z = F.relu(self.dropout(self.fc4(z)))\n",
    "        z = self.dropout(self.fc5(z))\n",
    "        \n",
    "        z_ = F.relu(self.dropout(self.fc3(z_)))\n",
    "        z_ = F.relu(self.dropout(self.fc4(z_)))\n",
    "        z_ = self.dropout(self.fc5(z_))\n",
    "        \n",
    "        return (x+x_), (y+y_), (z+z_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### train function ######\n",
    "\n",
    "def train_aux (model, train_data, mini_batch_size=100, optimizer = optim.SGD,\n",
    "                criterion = nn.CrossEntropyLoss(), n_epochs=50, eta=1e-1, lambda_l2 = 0, alpha=0.5, beta=0.5):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Train network with auxiliary loss + weight sharing\n",
    "    \n",
    "    \"\"\"\n",
    "    # create data loader\n",
    "    train_loader = DataLoader(train_data, batch_size=mini_batch_size, shuffle=True)\n",
    "    \n",
    "    model.train()\n",
    "    optimizer = optimizer(model.parameters(), lr = eta)\n",
    "    \n",
    "    for e in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            \n",
    "            input_, target_, classes_ = data\n",
    "            class_1, class_2,out= model(input_)\n",
    "            aux_loss1 = criterion(class_1, classes_[:,0])\n",
    "            aux_loss2 = criterion(class_2, classes_[:,1])\n",
    "            #aux_loss1_T = criterion(class_1_T, classes_[:,0])\n",
    "            #aux_loss2_T = criterion(class_2_T, classes_[:,1])\n",
    "            out_loss  = criterion(out, target_)\n",
    "            #out_T_loss = criterion(out_T, target_)\n",
    "            net_loss = (alpha * (out_loss) + beta * (aux_loss1 + aux_loss2 ))\n",
    "            epoch_loss += net_loss\n",
    "            \n",
    "            if lambda_l2 != 0:\n",
    "                for p in model.parameters():\n",
    "                    epoch_loss += lambda_l2 * p.pow(2).sum() # add an l2 penalty term to the loss \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            net_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print('Train Epoch: {}  | Loss {:.6f}'.format(\n",
    "                e, epoch_loss.item()))\n",
    "        \n",
    "#########################################################################################################################\n",
    "#########################################################################################################################\n",
    "\n",
    "### test function  ###\n",
    "\n",
    "def test_aux(model, test_data, mini_batch_size=100, criterion = nn.CrossEntropyLoss()):\n",
    "    \n",
    "    \"\"\"\n",
    "    Test function to calculate prediction accuracy of a cnn with auxiliary loss\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # create test laoder\n",
    "    test_loader = DataLoader(test_data, batch_size=mini_batch_size, shuffle=True)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    nb_errors=0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            input_, target_, classes_ = data\n",
    "            \n",
    "            _,_ ,output = model(input_) \n",
    "            batch_loss = criterion(output, target_)\n",
    "            test_loss += batch_loss\n",
    "            \n",
    "            _, predicted_classes = output.max(1)\n",
    "            for k in range(mini_batch_size):\n",
    "                if target_[k] != predicted_classes[k]:\n",
    "                    nb_errors = nb_errors + 1\n",
    "                                   \n",
    "             \n",
    "        print('\\nTest set | Loss: {:.4f} | Accuracy: {:.0f}% | # misclassified : {}/{}\\n'.format(\n",
    "        test_loss.item(), 100 * (len(test_data)-nb_errors)/len(test_data), nb_errors, len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0  | Loss 28.362577\n",
      "Train Epoch: 1  | Loss 8.908897\n",
      "Train Epoch: 2  | Loss 4.895871\n",
      "Train Epoch: 3  | Loss 2.951210\n",
      "Train Epoch: 4  | Loss 1.772059\n",
      "Train Epoch: 5  | Loss 1.030429\n",
      "Train Epoch: 6  | Loss 0.564917\n",
      "Train Epoch: 7  | Loss 0.336092\n",
      "Train Epoch: 8  | Loss 0.215338\n",
      "Train Epoch: 9  | Loss 0.146005\n",
      "Train Epoch: 10  | Loss 0.107445\n",
      "Train Epoch: 11  | Loss 0.082549\n",
      "Train Epoch: 12  | Loss 0.069765\n",
      "Train Epoch: 13  | Loss 0.058600\n",
      "Train Epoch: 14  | Loss 0.052406\n",
      "Train Epoch: 15  | Loss 0.048222\n",
      "Train Epoch: 16  | Loss 0.039775\n",
      "Train Epoch: 17  | Loss 0.035433\n",
      "Train Epoch: 18  | Loss 0.031399\n",
      "Train Epoch: 19  | Loss 0.028086\n",
      "Train Epoch: 20  | Loss 0.025468\n",
      "Train Epoch: 21  | Loss 0.023863\n",
      "Train Epoch: 22  | Loss 0.021607\n",
      "Train Epoch: 23  | Loss 0.019417\n",
      "Train Epoch: 24  | Loss 0.018275\n",
      "Train Epoch: 25  | Loss 0.016255\n",
      "Train Epoch: 26  | Loss 0.015382\n",
      "Train Epoch: 27  | Loss 0.014249\n",
      "Train Epoch: 28  | Loss 0.013081\n",
      "Train Epoch: 29  | Loss 0.012044\n",
      "Train Epoch: 30  | Loss 0.011323\n",
      "Train Epoch: 31  | Loss 0.010749\n",
      "Train Epoch: 32  | Loss 0.009890\n",
      "Train Epoch: 33  | Loss 0.009572\n",
      "Train Epoch: 34  | Loss 0.008777\n",
      "Train Epoch: 35  | Loss 0.008250\n",
      "Train Epoch: 36  | Loss 0.007792\n",
      "Train Epoch: 37  | Loss 0.006938\n",
      "Train Epoch: 38  | Loss 0.006446\n",
      "Train Epoch: 39  | Loss 0.006206\n",
      "Train Epoch: 40  | Loss 0.005821\n",
      "Train Epoch: 41  | Loss 0.005446\n",
      "Train Epoch: 42  | Loss 0.005442\n",
      "Train Epoch: 43  | Loss 0.004822\n",
      "Train Epoch: 44  | Loss 0.004649\n",
      "Train Epoch: 45  | Loss 0.004322\n",
      "Train Epoch: 46  | Loss 0.004112\n",
      "Train Epoch: 47  | Loss 0.003933\n",
      "Train Epoch: 48  | Loss 0.003580\n",
      "Train Epoch: 49  | Loss 0.003542\n",
      "\n",
      "Test set | Loss: 4.7025 | Accuracy: 94% | # misclassified : 64/1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = STN_Net_aux()\n",
    "train_aux(model, train_data,optimizer=optim.Adam,eta = 0.001)\n",
    "test_aux(model,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
